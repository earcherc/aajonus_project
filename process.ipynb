{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e05e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x164e69f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needles_Of_Disease_and_Death_Continue_In_The_Name_Of_Saving_Children.txt\n",
      "Diarrhea-based_Detoxification_Hotel_By_Medical_Doctors.txt\n",
      "The_FDA_Approved_5_Viruses_for_Food_Treatment.txt\n",
      "Genius_Children.txt\n",
      "Dr._Stanley_S._Bass_Interview.txt\n",
      "Q&A_Of_September_13,_2009.txt\n",
      "Causes_For_Most_Intestinal_Disease.txt\n",
      "Are_Raw_Miso_And_Shoyu_Healthy_Sauces?.txt\n",
      "Safe_Cutting_Boards.txt\n",
      "Multiple_Lacerations_Healed_Without_Medical_Help.txt\n",
      "Cholesterol,_LDL_and_HDL.txt\n",
      "Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt\n",
      "Can_We_Preserve_Raw_Chicken_In_Vinegar_Or_Lemon_Juice?.txt\n",
      "Abrasions,_Fractures_and_Breaks.txt\n",
      "Is_Raw_Chocolate_Made_From_Whole_Raw_Cocoa_Beans_Addictive_Or_Harmful?.txt\n",
      "What_Is_Constipation_And_How_Do_We_Resolve_It?.txt\n",
      "Our_Ubiquitous_Microbial_Friends.txt\n",
      "Quinton.txt\n",
      "Q&A_Of_December_14,_2008.txt\n",
      "Q&A_Of_October_14,_2012.txt\n",
      "My_Survival_Kit.txt\n",
      "Medical_Propaganda_about_Inflammatory_Breast_Cancer.txt\n",
      "How_Are_Nutrients_Delivered_To_Our_Cells?.txt\n",
      "Q&A_Of_August_24,_2008.txt\n",
      "Vaccines_Ruin_Your_Health.txt\n",
      "Frozen_Pastured_Meat_VS_Fresh_Supermarket_Meat.txt\n",
      "Long-term_Damage_From_Abduction_and_Forced_Injections.txt\n",
      "Q&A_Of_May_26,_2013.txt\n",
      "FLU_-_Viral_Tools_Improve_Health.txt\n",
      "Email_from_Aajonus_Summer_2012.txt\n",
      "Dental_Hygiene,_Causes_of_Decay_and_Reversal,_and_Re-enamelization.txt\n",
      "Bad_And_Good_Parasites,_And_Malaria?.txt\n",
      "View_on_Medical_Establishment.txt\n",
      "Formula_for_a_Healthy_Baby.txt\n",
      "Top_Aussie_Doctor_Says_Pick_Your_Nose_And_Eat_it.txt\n",
      "Supplements.txt\n",
      "Primal_Diet_Workshop_(Part_1).txt\n",
      "Raw_Dairy_Is_Healthy_But_Illegal.txt\n",
      "Does_Rabies_Exist?.txt\n",
      "Q&A_Of_May_2,_2004.txt\n",
      "Abduction_and_Injections.txt\n",
      "Is_Raw_Milk_Always_Beneficial_Even_With_Much_Bacteria?.txt\n",
      "Q&A_Of_September_10,_2006.txt\n",
      "How_Long_Does_It_Take_To_Understand_The_Primal_Diet(TM)?.txt\n",
      "Rawesome_Trial_Outcome.txt\n",
      "Q&A_Of_June_29,_2008.txt\n",
      "Whole_Foods_Markets,_Inc._Friend_To_Better_Health_Or_Foe?.txt\n",
      "Q&A_Of_December_17,_2006.txt\n",
      "What_Should_We_Consider_For_Health_When_Buying_A_New_Car?.txt\n",
      "Q&A_Of_January_24,_1999.txt\n",
      "Lobbying_in_Washington,_DC_for_Raw_Milk.txt\n",
      "What_Do_We_Do_About_Emerging_Plagues?.txt\n",
      "Is_The_Science_of_Viruses_Real?.txt\n",
      "Q&A_Of_May_20,_2012.txt\n",
      "Chemtrails.txt\n",
      "Q&A_Of_February_22,_2009.txt\n",
      "How_Can_EMFs_Cause_Death_Prematurely?.txt\n",
      "A_New_Theory_Of_Disease.txt\n",
      "Bruises,_Injuries_and_Pain_-_Do_We_Apply_Ice_Or_Heat?.txt\n",
      "Malaria.txt\n",
      "My_Research_And_Experiments_Questioned.txt\n",
      "Q&A_Of_February_18,_2007.txt\n",
      "TB_Testing_for_Teachers_and_Health_Practitioners.txt\n",
      "Primal_Diet_Is_Not_A_Stagnant_Diet.txt\n",
      "Protecting_From_Medical_Treatments_in_Emergencies.txt\n",
      "Care_To_Have_A_Piss_Of_A_Drink_With_Me?.txt\n",
      "Q&A_Of_February_20,_2005.txt\n",
      "Natural_Toys,_Oh,_My!.txt\n",
      "Plastic_consumption_might_lead_to_sexual_disorders.txt\n",
      "Q&A_Of_June_3,_2007.txt\n",
      "Q&A_Of_January_29,_2006.txt\n",
      "At_What_Age_Is_Death_Inevitable?.txt\n",
      "What_Is_Nutrient_Value_Of_Dehydrated_foods?.txt\n",
      "Loss_Of_My_BIOHAZARDS_Research.txt\n",
      "Interview_by_Mina_Olen,_Health_magazine_in_Finland..txt\n",
      "Q&A_Of_January_22,_2000.txt\n",
      "Q&A_Of_May_27,_2012.txt\n",
      "Long-term_Delayed_Detoxification;_58_Years_Later.txt\n",
      "A_Wonderful_Ruling_in_Communist_China_That_We_Should_Adopt.txt\n",
      "Q&A_Of_August_20,_2006.txt\n",
      "Will_We_Continually_Pay_for_Medical_Mass_Poisoning_of_Humanity?.txt\n",
      "More_Clarity_On_Food-borne_Bacterial_Contamination.txt\n",
      "Charlie_Donham_Interview.txt\n",
      "London_Times_Interview.txt\n",
      "Q&A_Of_April_14,_2002.txt\n",
      "Do_We_Believe_Medical_Studies?.txt\n",
      "Frozen_Fish_VS_Frozen_Land_Animals.txt\n",
      "Who_Has_The_Right_To_Institutionalize_Me?.txt\n",
      "Paul_Andrews_Interview.txt\n",
      "Mercury_In_Fish;_Do_We_Absorb_It?.txt\n",
      "Benefits_of_Raw_Eggs.txt\n",
      "Q&A_Of_March_11,_2012.txt\n",
      "Study_Of_Thyroid_Cancer.txt\n",
      "Q&A_Of_September_19,_2000_(Rare).txt\n",
      "Vaccines;_Nice_Shots_Or_Not.txt\n",
      "What_Are_Drugs_And_Supplements?.txt\n",
      "What_Water_Should_I_Buy?.txt\n",
      "Vaccines,_All_Harmful_Or_Some_Beneficial?.txt\n",
      "Are_Citizens_Being_Attacked_By_Their_Governments?.txt\n",
      "What_Is_Our_Likelihood_Of_Developing_Cancer(s)?.txt\n",
      "Q&A_Of_June_10,_2007_&_September_9,_2007.txt\n",
      "Q&A_Of_September_9,_2007.txt\n",
      "Chemicals_Used_to_Protect_Food_From_Bacteria;_Harmful.txt\n",
      "Salt_And_Headaches.txt\n",
      "Human_Papillomavirus_(HPV)_Vaccine.txt\n",
      "Q&A_Of_May_24,_2009.txt\n",
      "Q&A_Of_November_17,_2000.txt\n",
      "Toxic_Chemicals_Out-Gassing_Into_Our_Homes.txt\n",
      "Corporate_&_Government_Tyranny_Behind_Disease.txt\n",
      "Q&A_Of_September_11,_2011.txt\n",
      "Q&A_Of_February_1,_2004.txt\n",
      "Q&A_Of_March_17,_2002.txt\n",
      "Iridology.txt\n",
      "Medications_Are_Toxic_And_Store_In_The_Tissue.txt\n",
      "H1N1_(Swine)_Flu_Epidemic,_Fact_or_Hoax?.txt\n",
      "Q&A_Of_May_30,_2010.txt\n",
      "How_Much_Bacteria_Are_We_Today?.txt\n",
      "LA_Times_Article_About_Raw-Foods.txt\n",
      "Superfoods.txt\n",
      "Why_Mercury_Is_Not_Absorbed_When_We_Eat_Raw_Fish_(Theory).txt\n",
      "Q&A_Of_April_17,_2005.txt\n",
      "Does_Raw_Milk_Do_A_Body_Good?.txt\n",
      "Q&A_Of_July_10,_2011.txt\n",
      "Q&A_Of_November_14,_2004.txt\n",
      "Exercise;_The_Good,_Bad_and_Beautiful.txt\n",
      "How_Much_Energy_Should_I_Expect_To_Experience?.txt\n",
      "Q&A_Of_August_17,_2003_(Full).txt\n",
      "What_Would_Happen_If_Aajonus_Ate_Some_Cooked_Meat?.txt\n",
      "About.txt\n",
      "Eating_Out,_Is_It_Safe?.txt\n",
      "Resolving_Early_Morning_Racing_Mind.txt\n",
      "Medical_Researchers_Proved_90%_Medical_Research_Is_False.txt\n",
      "Fermented_Vegetables;_The_Good,_Bad_and_Stinky.txt\n",
      "Q&A_Of_April_6,_2008.txt\n",
      "Q&A_Of_January_27,_2013.txt\n",
      "Gallbladder_Stones.txt\n",
      "Q&A_Of_July_20,_2008.txt\n",
      "Q&A_Of_May_29,_2011.txt\n",
      "Quality_or_Quantity?.txt\n",
      "Q&A_Of_May_21,_2000_(Rare).txt\n",
      "Q&A_Of_March_19,_2006.txt\n",
      "Grow_In_Height_After_Age_21.txt\n",
      "Q&A_Of_April_11,_2004.txt\n",
      "Pickled_Fish.txt\n",
      "The_Recipe_for_Living_Without_Disease.txt\n",
      "Q&A_Of_February_9,_2003.txt\n",
      "Severe_Back_Deterioration.txt\n",
      "How_To_Use_An_EMF_Meter.txt\n",
      "Interview_on_Talksport.net.txt\n",
      "Q&A_Of_November_27,_2005.txt\n",
      "Q&A_Of_January_9,_2011.txt\n",
      "Depression.txt\n",
      "Home-grown_Vegetables_Blamed_For_Disease.txt\n",
      "Ingredients_In_Injections_With_Hair_And_Iridology_Analyses.txt\n",
      "Q&A_Of_September_12,_2005.txt\n",
      "Dissolving_Plaque_from_Our_Circulatory_Systems.txt\n",
      "Why_Do_Most_Physicians_Refuse_Chemo-treatments?.txt\n",
      "How_Toxic_is_Our_Civilized_World?.txt\n",
      "Child_Cured_by_Mothers_Feces_-_Eat_Shit_and_Live!.txt\n",
      "Oysters_-_Special_Food_In_Our_Toxic_World.txt\n",
      "How_To_Remove_Fear_Of_Microbes.txt\n",
      "Q&A_Of_June_16,_2013.txt\n",
      "Q&A_Of_September_14,_2003.txt\n",
      "How_Do_Electromagnetic_Fields_Affect_Us?.txt\n",
      "Q&A_Of_September_12,_2004.txt\n",
      "What_Is_Nutrient_Value_Trace_Minerals?.txt\n",
      "Q&A_Of_February_25,_2007.txt\n",
      "Will_Big_Industries_Control_Us_via_Government?.txt\n",
      "Considering_Chemotherapy_As_An_Option_For_Cancer?.txt\n",
      "Q&A_Of_July_8,_2001.txt\n",
      "Q&A_Of_August_22,_2011.txt\n",
      "Blood_Types_For_Meat.txt\n",
      "Primal_Diet_Workshop_+_Q&A_Of_June_22,_2013.txt\n",
      "Baby_Food_For_Mothers_Who_Can't_Breastfeed.txt\n",
      "Q&A_Of_July_24,_2005.txt\n",
      "Bacteria,_Antibiotics,_Immune_System.txt\n",
      "Q&A_Of_March_26,_2000.txt\n",
      "Ball_and_Kerr_Jar_Lids,_Are_They_Plastic_Coated_and_Toxic_or_Not?.txt\n",
      "Q&A_Of_June_22,_2003.txt\n",
      "Does_Food_Affect_Behavior?.txt\n",
      "Q&A_Of_November_18,_2007.txt\n",
      "Q&A_Of_April_16,_2006.txt\n",
      "Friendly_Bacteria_Protect_Against_Type_1_Diabetes.txt\n",
      "Are_There_Aggressive_Treatments_For_Cancer?.txt\n",
      "Microbe_Food-Poisoning;_Fact_or_Fiction?.txt\n",
      "Primal_Diet_Workshop_(Part_2).txt\n",
      "What_Place_Do_Energy_Therapies_Take_In_Healing?.txt\n",
      "Chemical_Burns_Can_Be_Devastating_But_Managed_And_Healed.txt\n",
      "E.coli.txt\n",
      "Effects_of_Dietary_and_Environmental_Pollution_on_Children's_Sleep.txt\n",
      "Study_Shows_That_Chubby_People_Live_Longest.txt\n",
      "Q&A_Of_March_18,_2012.txt\n",
      "Oxalates.txt\n",
      "Quick_Alternative_Cure_For_Arthritis;_True_Or_False?.txt\n",
      "Is_It_True_You_Eat_Buckets_Of_Cow_Dung?.txt\n",
      "Can_We_Preserve_Raw_Fish_In_Oil?.txt\n",
      "100%_Of_Fresh-Water_Lakes_And_Streams_Are_Polluted_With_Mercury.txt\n",
      "Q&A_Of_May_7,_2006.txt\n",
      "Non-organic_Meat.txt\n",
      "Coronavirus.txt\n",
      "Repeated_Surgeries_Resulted_In_Thick_Scars.txt\n",
      "Q&A_Of_October_24,_2010.txt\n",
      "Question_And_Answers.txt\n",
      "Man_Eats_Live_Frogs_and_Rats_for_Health.txt\n",
      "Dangers_Of_Salt.txt\n",
      "Iron_On_The_Primal_Diet,_Is_It_A_Problem?.txt\n",
      "Q&A_Of_February_3,_2013.txt\n",
      "Q&A_Of_March_26,_2013.txt\n",
      "FDA_Approves_6_Viruses_As_Safe.txt\n",
      "Beneficial_Home_Baths.txt\n",
      "New_Source_Of_Stem_Cells_-_Mouse_Sperm.txt\n",
      "Athletes_And_Longevity_On_Primal_Diet.txt\n",
      "Q&A_Of_August_17,_2003.txt\n",
      "What_Role_Do_Genetics_and_Microbes_Play_In_Disease?.txt\n",
      "Benzene,_Cancer_and_Soft_Drinks_Connection.txt\n",
      "Primal_Diet_Workshop_(Part_3).txt\n",
      "High_Blood-Pressure.txt\n",
      "Q&A_Of_May_23,_2010.txt\n",
      "Rae_Bradbury_Interview_1.txt\n",
      "Is_It_Good_To_Donate_To_Charities_That_Feed_The_Poor_and_Starving?.txt\n",
      "Q&A_Of_April_4,_2010.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With_Mercury_Found_In_Wild_Animals,_Do_We_Need_To_Be_Extra_Careful?.txt\n",
      "Q&A_Of_September_26,_2010.txt\n",
      "Do_You_Buy_Chicken_While_Traveling?.txt\n",
      "Cancer_Convention_September_2000.txt\n",
      "Q&A_Of_November_7,_1999.txt\n",
      "Q&A_Of_November_26,_2006.txt\n",
      "How_Bad_Are_MRIs?.txt\n",
      "Arsenic_In_Poultry_Meat_And_Eggs.txt\n",
      "Joanne_Unleahsed_Interview.txt\n",
      "Declaring_Our_Rights_To_Our_Body.txt\n",
      "We_Want_To_Live.txt\n",
      "Soy_Toxicity_In_Poultry_Meat_And_Eggs.txt\n",
      "Hot_Tub_Therapy.txt\n",
      "Bacteria_and_Other_Microbes_Are_Responsible_for_Vibrant_Health.txt\n",
      "Gum_And_Tooth_Disease.txt\n",
      "Rae_Bradbury_Interview_2.txt\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \n",
      "0  On Halloween, I received the most alarming ter...  \n",
      "1  I received it\\nin a letter from Care2 organiza...  \n",
      "2  Most of us have\\nnever witnessed the crippling...  \n",
      "3  Polio is still endemic in three of the world's...  \n",
      "4  This is the scary truth: levels of polio are a...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"aajonus_data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"aajonus_saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files\n",
    "# If yes, then reinitialise these as dfs\n",
    "# If not, then create the dfs and save them in csv format for next run\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            # Create the filepath\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # Use spaCy to tokenize the content into sentences\n",
    "                doc = nlp(content)\n",
    "                sentences = [sent.text.strip() for sent in doc.sents]\n",
    "                # Append each sentence to your data list, along with the filename\n",
    "                for sentence in sentences:\n",
    "                    data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save DF\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethancavill/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "JOBLIB_DIR = Path.cwd() / \"aajonus_joblibs\"\n",
    "JOBLIB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "vectorizer_path = JOBLIB_DIR / 'tfidf_vectorizer.joblib'\n",
    "matrix_path = JOBLIB_DIR / 'tfidf_matrix.joblib'\n",
    "\n",
    "max_df = 0.7\n",
    "min_df = 0.00\n",
    "ngram_range = (1, 1)\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Check if parameters have changed and files exist\n",
    "params_changed = False\n",
    "if vectorizer_path.exists():\n",
    "    existing_vectorizer = joblib.load(vectorizer_path)\n",
    "    if (existing_vectorizer.max_df != max_df or \n",
    "        existing_vectorizer.min_df != min_df or \n",
    "        existing_vectorizer.ngram_range != ngram_range):\n",
    "        params_changed = True\n",
    "        os.remove(vectorizer_path)\n",
    "        os.remove(matrix_path)\n",
    "\n",
    "if not matrix_path.exists() or params_changed:\n",
    "    print(\"Fitting TF-IDF vectorizer to the dataset...\")\n",
    "    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['sentence'])\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    joblib.dump(tfidf_matrix, matrix_path)\n",
    "else:\n",
    "    print(\"Loading fitted TF-IDF vectorizer and matrix dataset...\")\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "    tfidf_matrix = joblib.load(matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, vectorizer, tfidf_matrix, df):\n",
    "    query_vector = vectorizer.transform([query])  # Preprocessing is handled by vectorizer\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    top_indices = similarities.argsort()[0][-5:]\n",
    "\n",
    "    # Retrieve the corresponding rows from the DataFrame\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    top_scores = similarities[0][top_indices]\n",
    "\n",
    "    return top_docs, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Relevance Score\", \"Filename\", \"Date\", \"Max DF\", \"Min DF\", \"Ngram Range\"]\n",
    "\n",
    "def search_main(query, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range, test_set):\n",
    "    start_time = time.time()\n",
    "    top_docs, top_scores = search(query, vectorizer, tfidf_matrix, df)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    new_rows = []\n",
    "    for index, score in zip(top_docs.index, top_scores):\n",
    "        print(f\"\\n'{query}': '{top_docs.loc[index]['sentence']}', [{score}]\")\n",
    "        row = top_docs.loc[index]\n",
    "        new_rows.append({\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "            \"Relevance Score\": None,\n",
    "            \"Date\": pd.Timestamp('now'),\n",
    "            \"Max DF\": max_df,\n",
    "            \"Min DF\": min_df,\n",
    "            \"Ngram Range\": ngram_range\n",
    "        })\n",
    "\n",
    "    test_set = pd.concat([test_set, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61276110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'% juice': 'Juice it.', [0.6138162224846807]\n",
      "\n",
      "'% juice': 'Juice it.', [0.6138162224846807]\n",
      "\n",
      "'% juice': 'And the juice I recommend for you is\n",
      "80% celery, 10% parsley, 10% carrot juice.', [0.649128424930602]\n",
      "\n",
      "'% juice': 'Juices.', [0.6803939690038127]\n",
      "\n",
      "'% juice': 'Juice.', [0.6803939690038127]\n"
     ]
    }
   ],
   "source": [
    "TEST_SET_DIR = Path.cwd() / \"aajonus_test_sets\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_test_set_df(test_set_path):\n",
    "    if test_set_path.exists():\n",
    "        df = pd.read_csv(test_set_path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])  # Convert to datetime\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame(columns=test_set_columns)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / \"test_set_v1.csv\"\n",
    "test_set = get_test_set_df(test_set_path)\n",
    "\n",
    "query = \"% juice\"\n",
    "test_set = search_main(query, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range, test_set)\n",
    "\n",
    "# Save updated DataFrame\n",
    "# test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Relevance: [0 0 1 1 1]\n",
      "Cosine scores: [0.49779978 0.5265989  0.56118028 0.58875345 0.84168432]\n",
      "Predicted Relevance: [0 1 1 1 1]\n",
      "Precision: 0.75\n",
      "Recall: 1.0\n",
      "F1: 0.8571428571428571\n",
      "True Relevance: [0 0 0 0 1]\n",
      "Cosine scores: [0.49219966 0.58991814 0.59057851 0.593077   0.60237156]\n",
      "Predicted Relevance: [0 1 1 1 1]\n",
      "Precision: 0.25\n",
      "Recall: 1.0\n",
      "F1: 0.4\n",
      "True Relevance: [0 0 0 0 1]\n",
      "Cosine scores: [0.56733498 0.56733498 0.56733498 0.56733498 0.65710913]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "F1: 0.33333333333333337\n",
      "True Relevance: [0 1 0 0 1]\n",
      "Cosine scores: [0.73168746 0.76989938 0.77023495 0.86038613 0.90681405]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "F1: 0.5714285714285715\n",
      "True Relevance: [0 1 1 0 1]\n",
      "Cosine scores: [0.53737544 0.54971306 0.55680845 0.6005335  0.63971631]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [0 0 0 0 1]\n",
      "Cosine scores: [0.62174949 0.65467818 0.66367551 0.73808873 0.74559623]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "F1: 0.33333333333333337\n",
      "True Relevance: [0 0 0 0 0]\n",
      "Cosine scores: [0.75776636 0.77035143 0.81093586 0.83754904 0.93598669]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [1 0 0 0 0]\n",
      "Cosine scores: [0.58555909 0.6077594  0.61178264 0.63466454 0.66984826]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "F1: 0.33333333333333337\n",
      "True Relevance: [0 1 1 0 1]\n",
      "Cosine scores: [0.61858597 0.62482419 0.69849299 0.70754274 0.90999723]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [0 1 1 0 1]\n",
      "Cosine scores: [0.45413501 0.4653836  0.47423326 0.49976128 0.6139637 ]\n",
      "Predicted Relevance: [0 0 0 0 1]\n",
      "Precision: 1.0\n",
      "Recall: 0.3333333333333333\n",
      "F1: 0.5\n",
      "True Relevance: [0 1 1 1 1]\n",
      "Cosine scores: [0.63331929 0.69812429 0.71339034 0.75970119 0.76646848]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.8\n",
      "Recall: 1.0\n",
      "F1: 0.888888888888889\n",
      "True Relevance: [0 1 0 0 0]\n",
      "Cosine scores: [0.56293167 0.62053419 0.63410356 0.67526738 0.68128255]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "F1: 0.33333333333333337\n",
      "True Relevance: [0 1 1 0 1]\n",
      "Cosine scores: [0.52497558 0.54429727 0.55032216 0.55322903 0.64250758]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [0 0 0 1 0]\n",
      "Cosine scores: [0.40413591 0.43868313 0.4465315  0.50368351 0.67765882]\n",
      "Predicted Relevance: [0 0 0 1 1]\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1: 0.6666666666666666\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.71556067 0.74325192 0.74744469 0.75917597 0.88212843]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "True Relevance: [0 0 0 1 1]\n",
      "Cosine scores: [0.52111905 0.52111905 0.5214393  0.57372565 0.59861603]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.4\n",
      "Recall: 1.0\n",
      "F1: 0.5714285714285715\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.37649597 0.41325368 0.4247664  0.45636362 0.48894798]\n",
      "Predicted Relevance: [0 0 0 0 0]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [1 1 0 1 1]\n",
      "Cosine scores: [0.55613847 0.55875481 0.56341687 0.60714593 0.72953279]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.8\n",
      "Recall: 1.0\n",
      "F1: 0.888888888888889\n",
      "True Relevance: [1 1 0 0 1]\n",
      "Cosine scores: [0.75488971 0.7571916  0.78441805 0.78441805 0.78934535]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [0 0 0 0 0]\n",
      "Cosine scores: [0.5537353  0.56336337 0.57222212 0.57222212 0.59650979]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [0 1 1 0 1]\n",
      "Cosine scores: [0.54644001 0.59039634 0.59039634 0.66157749 0.67691595]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [0 0 1 0 0]\n",
      "Cosine scores: [0.38968111 0.39020296 0.44511877 0.46981008 0.47242056]\n",
      "Predicted Relevance: [0 0 0 0 0]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [1 1 1 0 1]\n",
      "Cosine scores: [0.35025118 0.35082141 0.35222057 0.43743274 0.46229355]\n",
      "Predicted Relevance: [0 0 0 0 0]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [0 0 0 1 0]\n",
      "Cosine scores: [0.55715175 0.55808461 0.57484442 0.59305594 0.68283634]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.2\n",
      "Recall: 1.0\n",
      "F1: 0.33333333333333337\n",
      "True Relevance: [0]\n",
      "Cosine scores: [0.9194564]\n",
      "Predicted Relevance: [1]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [0 1 0 0]\n",
      "Cosine scores: [0.65654026 0.66633896 0.69416164 0.74187323]\n",
      "Predicted Relevance: [1 1 1 1]\n",
      "Precision: 0.25\n",
      "Recall: 1.0\n",
      "F1: 0.4\n",
      "True Relevance: [0 0 1 1 1]\n",
      "Cosine scores: [0.62658312 0.64066098 0.76902681 0.77845215 0.77845215]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.6\n",
      "Recall: 1.0\n",
      "F1: 0.7499999999999999\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.77872849 0.79915347 0.80228438 0.8314604  0.89288687]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.73342672 0.75282061 0.81897811 0.82638391 0.84720746]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "True Relevance: [0 0 0 0 0]\n",
      "Cosine scores: [0. 0. 0. 0. 0.]\n",
      "Predicted Relevance: [0 0 0 0 0]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.6000891  0.60602116 0.66193642 0.68696254 0.71101566]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "True Relevance: [1 1 1 1 0]\n",
      "Cosine scores: [0.49023052 0.56369909 0.5731134  0.59367348 0.6535666 ]\n",
      "Predicted Relevance: [0 1 1 1 1]\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1: 0.75\n",
      "True Relevance: [1 1 1 1 1]\n",
      "Cosine scores: [0.52596423 0.56277686 0.67058404 0.67678528 0.7144309 ]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n",
      "True Relevance: [0 0 0 0 0]\n",
      "Cosine scores: [0.67062001 0.67661134 0.69104985 0.70875084 0.82021392]\n",
      "Predicted Relevance: [1 1 1 1 1]\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "                                        Query  Precision  Recall  F1-Score\n",
      "0                                Child genius       0.75     1.0  0.857143\n",
      "1              What's the ideal diet routine?       0.25     1.0  0.400000\n",
      "2                      Fastest way to get fat       0.20     1.0  0.333333\n",
      "3                          How to gain weight       0.40     1.0  0.571429\n",
      "4  Milkshake vs lubricator for gaining weight       0.60     1.0  0.750000\n"
     ]
    }
   ],
   "source": [
    "# After creating the test_set and evaluating the relevance of the top 5, we now need to compute some metrics. \n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_evaluations(test_set, threshold=0.5):\n",
    "    evaluation_data = []\n",
    "    \n",
    "    # This loops over each unique column name\n",
    "    for query in test_set['Query'].unique():\n",
    "        # Then we create a new df which filters the test_set on the query col\n",
    "        # This df will be 5 rows\n",
    "        current_query_data = test_set[test_set['Query'] == query]\n",
    "        \n",
    "        # We then extract out the columns and create numpy arrays from the respective values \n",
    "        true_relevance = current_query_data['Relevance Score'].to_numpy()\n",
    "        cosine_scores = current_query_data['Cosine'].to_numpy()\n",
    "        \n",
    "        # Convert cosine scores to binary predictions\n",
    "        predicted_relevance = (cosine_scores >= threshold).astype(int)\n",
    "        \n",
    "        # print(f\"True Relevance: {true_relevance}\")\n",
    "        # print(f\"Cosine scores: {cosine_scores}\")\n",
    "        # print(f\"Predicted Relevance: {predicted_relevance}\")\n",
    "        \n",
    "        # Calculate precision, recall, and F1-score with zero_division parameter\n",
    "        # Precison: True Pos / (True Pos + False Pos)\n",
    "        # Recall: True Pos / (True Pos + False Neg)\n",
    "        # Harmonic mean: 2 * (Prec x Rec / (Prec + Rec))\n",
    "        precision = precision_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        recall = recall_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        f1 = f1_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        \n",
    "        # print(f\"Precision: {precision}\")\n",
    "        # print(f\"Recall: {recall}\")\n",
    "        # print(f\"F1: {f1}\")\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query': query,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"aajonus_evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / \"test_set_v1.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set)\n",
    "eval_path = EVAL_DIR / \"evaluation_v1.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb85d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
