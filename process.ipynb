{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e05e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x163d0a510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset from CSV...\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \n",
      "0  On Halloween, I received the most alarming ter...  \n",
      "1  I received it\\nin a letter from Care2 organiza...  \n",
      "2  Most of us have\\nnever witnessed the crippling...  \n",
      "3  Polio is still endemic in three of the world's...  \n",
      "4  This is the scary truth: levels of polio are a...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"aajonus_data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"aajonus_saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "full_df_path = DF_DIR / \"full_dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files\n",
    "# If yes, then reinitialise these as dfs\n",
    "# If not, then create the dfs and save them in csv format for next run\n",
    "if full_df_path.exists():\n",
    "    print(\"Loading full dataset from CSV...\")\n",
    "    df = pd.read_csv(full_df_path)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            # Create the full filepath\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # Use spaCy to tokenize the content into sentences\n",
    "                doc = nlp(content)\n",
    "                sentences = [sent.text.strip() for sent in doc.sents]\n",
    "                # Append each sentence to your data list, along with the filename\n",
    "                for sentence in sentences:\n",
    "                    data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save DF\n",
    "    df.to_csv(full_df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ae3e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset from CSV...\n",
      "                                        filename  \\\n",
      "0  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "1  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "2  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "3  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "4  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "\n",
      "                                            sentence  \n",
      "0  @Source\\n\\nTranscriber: Michael - Thank you, M...  \n",
      "1  Primal Diet Workshop in Nevada City, Californi...  \n",
      "2  He's here to talk to us\\nabout raw food, about...  \n",
      "3  Aajonus came into our\\nlives a couple of years...  \n",
      "4                                  Thank you Jill. [  \n"
     ]
    }
   ],
   "source": [
    "test_set_queries_path = Path.cwd() / \"aajonus_test_set_data\" / \"aajonus_test_set_data.csv\"\n",
    "test_df_path = DF_DIR / \"test_dataframe.csv\"\n",
    "\n",
    "labelled_queries_df = pd.read_csv(test_set_queries_path)\n",
    "\n",
    "if test_df_path.exists():\n",
    "    print(\"Loading test dataset from CSV...\")\n",
    "    test_set_df = pd.read_csv(test_df_path)\n",
    "else:\n",
    "    unique_filenames = labelled_queries_df['Filename'].unique()\n",
    "    # Create a copy of the filtered DataFrame to ensure it's independent\n",
    "    test_set_df = df[df['filename'].isin(unique_filenames)].copy()\n",
    "\n",
    "    test_set_df.to_csv(test_df_path, index=False)\n",
    "\n",
    "print(test_set_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95247a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "# Lemmatise token, proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0278fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed full dataset from CSV...\n",
      "Loading preprocessed test dataset from CSV...\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "5  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "6  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "7  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "8  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "9  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  On Halloween, I received the most alarming ter...   \n",
      "1  I received it\\nin a letter from Care2 organiza...   \n",
      "2  Most of us have\\nnever witnessed the crippling...   \n",
      "3  Polio is still endemic in three of the world's...   \n",
      "4  This is the scary truth: levels of polio are a...   \n",
      "5  Eradication is\\nwithin reach, but we need your...   \n",
      "6  In fact, if we don't end polio now,\\nit could ...   \n",
      "7  The United Nations Foundation's Shot@Life camp...   \n",
      "8                                We're almost there!   \n",
      "9  It costs less than $1 to vaccinate a child aga...   \n",
      "\n",
      "                            expanded_lemmatized_text  \n",
      "0  on halloween , I receive the most alarming ter...  \n",
      "1  I receive it \\n in a letter from care2 organiz...  \n",
      "2  most of we have \\n never witness the crippling...  \n",
      "3  polio be still endemic in three of the world '...  \n",
      "4  this be the scary truth : level of polio be at...  \n",
      "5  eradication be \\n within reach , but we need y...  \n",
      "6  in fact , if we do not end polio now , \\n it c...  \n",
      "7  the united nations foundation 's shot@life cam...  \n",
      "8                               we be almost there !  \n",
      "9  it cost less than $ 1 to vaccinate a child aga...  \n",
      "                                        filename  \\\n",
      "0  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "1  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "2  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "3  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "4  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "5  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "6  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "7  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "8  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "9  Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  @Source\\n\\nTranscriber: Michael - Thank you, M...   \n",
      "1  Primal Diet Workshop in Nevada City, Californi...   \n",
      "2  He's here to talk to us\\nabout raw food, about...   \n",
      "3  Aajonus came into our\\nlives a couple of years...   \n",
      "4                                  Thank you Jill. [   \n",
      "5                                    Clapping]Hello!   \n",
      "6                        Has everyone read the book?   \n",
      "7  Or are there some people who haven't read the ...   \n",
      "8                  Anybody who hasn't read the book?   \n",
      "9                                          4 people.   \n",
      "\n",
      "                            expanded_lemmatized_text  \n",
      "0  @source \\n\\n transcriber : michael - thank you...  \n",
      "1  primal diet workshop in nevada city , californ...  \n",
      "2  he be here to talk to we \\n about raw food , a...  \n",
      "3  aajonus come into our \\n live a couple of year...  \n",
      "4                                 thank you jill . [  \n",
      "5                                   clapping]hello !  \n",
      "6                      have everyone read the book ?  \n",
      "7  or be there some people who have not read the ...  \n",
      "8               anybody who have not read the book ?  \n",
      "9                                         4 people .  \n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DF_DIR = Path.cwd() / \"processed_data\"\n",
    "PROCESSED_DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "full_processed_df_path = PROCESSED_DF_DIR / \"full_dataset_preprocessed.csv\"\n",
    "test_processed_df_path = PROCESSED_DF_DIR / \"test_set_preprocessed.csv\"\n",
    "\n",
    "# Check and preprocess full dataset\n",
    "if full_processed_df_path.exists():\n",
    "    print(\"Loading preprocessed full dataset from CSV...\")\n",
    "    full_df = pd.read_csv(full_processed_df_path)\n",
    "else:\n",
    "    df['expanded_lemmatized_text'] = df['sentence'].apply(spacy_lemmatize)\n",
    "    df.to_csv(full_processed_df_path, index=False)\n",
    "    full_df = df\n",
    "\n",
    "# Check and preprocess test dataset\n",
    "if test_processed_df_path.exists():\n",
    "    print(\"Loading preprocessed test dataset from CSV...\")\n",
    "    test_set_df = pd.read_csv(test_processed_df_path)\n",
    "else:\n",
    "    test_set_df['expanded_lemmatized_text'] = test_set_df['sentence'].apply(spacy_lemmatize)\n",
    "    test_set_df.to_csv(test_processed_df_path, index=False)\n",
    "\n",
    "print(full_df.head(10))\n",
    "print(test_set_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fitted TF-IDF vectorizer and matrix for full dataset...\n",
      "Loading TF-IDF matrix for test dataset...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "JOBLIB_DIR = Path.cwd() / \"aajonus_joblibs\"\n",
    "JOBLIB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "vectorizer_path = JOBLIB_DIR / 'tfidf_vectorizer.joblib'\n",
    "full_matrix_path = JOBLIB_DIR / 'tfidf_full_matrix.joblib'\n",
    "test_matrix_path = JOBLIB_DIR / 'tfidf_test_matrix.joblib'\n",
    "\n",
    "max_df = 0.4\n",
    "min_df = 0.00\n",
    "ngram_range = (1, 4)\n",
    "# Tokenizer \n",
    "\n",
    "\n",
    "# Check if parameters have changed and files exist\n",
    "params_changed = False\n",
    "if vectorizer_path.exists():\n",
    "    existing_vectorizer = joblib.load(vectorizer_path)\n",
    "    if (existing_vectorizer.max_df != max_df or \n",
    "        existing_vectorizer.min_df != min_df or \n",
    "        existing_vectorizer.ngram_range != ngram_range):\n",
    "        params_changed = True\n",
    "        os.remove(vectorizer_path)\n",
    "        os.remove(full_matrix_path)\n",
    "        if test_matrix_path.exists():\n",
    "            os.remove(test_matrix_path)\n",
    "\n",
    "# Check if the TF-IDF matrix for the full dataset already exists\n",
    "if not full_matrix_path.exists() or params_changed:\n",
    "    print(\"Fitting TF-IDF vectorizer to the full dataset...\")\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "    tfidf_full_matrix = vectorizer.fit_transform(full_df['expanded_lemmatized_text'])\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    joblib.dump(tfidf_full_matrix, full_matrix_path)\n",
    "else:\n",
    "    print(\"Loading fitted TF-IDF vectorizer and matrix for full dataset...\")\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "    tfidf_full_matrix = joblib.load(full_matrix_path)\n",
    "\n",
    "# Process the test dataset\n",
    "if not test_matrix_path.exists() or params_changed:\n",
    "    print(\"Transforming test dataset using fitted vectorizer...\")\n",
    "    tfidf_test_matrix = vectorizer.transform(test_set_df['expanded_lemmatized_text'])\n",
    "    joblib.dump(tfidf_test_matrix, test_matrix_path)\n",
    "else:\n",
    "    print(\"Loading TF-IDF matrix for test dataset...\")\n",
    "    tfidf_test_matrix = joblib.load(test_matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, vectorizer, tfidf_matrix, df):\n",
    "    # Don't use this. All part of the vectoriser. Same NLP Pipeline \n",
    "    preprocessed_query = spacy_lemmatize(query)\n",
    "    \n",
    "    query_vector = vectorizer.transform([preprocessed_query])\n",
    "    \n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get the top 10 most similar document indices\n",
    "    top_indices = similarities.argsort()[0][-10:]\n",
    "    \n",
    "    # Return the most similar documents and their similarity scores\n",
    "    return df.iloc[top_indices], similarities[0][top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd602d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Genius children\n",
      "Expected Sentence: All hyperactive children have potential genius, but unless they can utilize proteins or harness adrenaline, their genius may turn into antisocial behavior.\n",
      "Retrieved Sentences: ['I had jaundice when I was a child.', \"A: So there you've got a difficult child, you know that child's not going to be\\nmindful.\", 'I looked like a mad child.', 'They have child after child simply because they want to.', 'The\\nchild repaired faster than any other child.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.13612103462219238s\n",
      "\n",
      "Query: Lubrication formula recipe\n",
      "Expected Sentence: 1 to 2 raw eggs 2 to 4 ounces unsalted raw butter or coconut cream 1 to 2 tablespoons lemon juice 1 to 2 teaspoons unheated honey\n",
      "Retrieved Sentences: [\"See acid/alkaline balance\\nAmbrosia Coconut Cream Pie (recipe), 134–135\\nAmbrosia Cream Pie (recipe), 136–137\\namino acids, destruction of, 157\\nanaphylaxis, 174\\nanimals\\nE. coli consumption by, 175\\neffect of processed foods on, 157\\nand Pottenger's raw diet, 165–166\\nantibiotics, 16, 156, 174-175\\nAntiperspirant, Natural (formula), 147\\nArab, Dr. Sara, 170–171\\narterial/intestinal plaque removal (recipe), 55\\nAsian Spicy Meat Sauce (recipe), 62\\nasthma, 182, 185\\nautoimmune inoculation, 18\\navocados, combining as fruit, 36\\n\\nbaby food recipes, 43, 51–53\\nInfant Glandular Booster, 52\\nInfant Immune Booster, 52\\nInfant Milkshake, 53\\nInfant Nervous System Booster, 53\\nbacteria\\nas detoxifying agents, 23\\nas disease eliminators, 170–174\\nfallacy of war on, 168–169\\nfalse concern over, 18–20, 177–178\\nfood poisoning from, 174–177, 181, 185\\nfutility of eliminating, 179-180\\n\\nInde x 191\\n\\nintestinal , low levels of , 148\\nlo w levels f rom drugs & chemicals, 188\\nbalanced diet, 26\\nBanana Cream Pie (recipe), 138–139\\nBanana Smoothie (recipe), 58\\nBarbecue Sauce (recipe), 63\\nbeauty formulas, topical, 145–147\\nMoisturizing/Lubrication Formul a Drink, 146\\nNatural Antiperspir ant, 147\\nNatural Deodorant, 147\\nPrimal Facial Body Care Cream, 145\\nBéchamel Sauce (recipe), 63\\nBechamp, Dr. Antoine, 162–164\\nbee pollen, optimum temperatures for, 26\\nBeef Pâté (recipe), 89\\nBeef Stroganoff (r ecipe), 89\\nbeet juice, 30\\nBerry Good Ice Cream (recipe), 131\\nbeverage recipe s, 54–59\\nBanana Smoothie, 58\\nCoffee Substitute, 57\\nGreen Vegetable Juices, 54–56\\nMilkshake, 5 7\\nOrange Smoothie, 58\\nRaspberry Smoothie, 59\\nBl and-Fruit Sala d (recipe), 142\\nblender technique, 46–47\\nblood\\nalkalinity of, 30\\nfat levels i n, 32-33, 157\\nsugar levels, 30, 32\\nbody salts regul ation/oxygen absorption (formulas), 54–56\\nbone marrow in recipes, 64, 92\\nBordelaise Sauce (recipes), 64\\nbowel disease, inflammatory, 173\\nbowel movements, 24, 148\\nbrains\\nraw (recipe), 1 04\\ntumors in, 171\\nbutter, raw\\nconstipation and, 24\\ndissolving and binding with toxins, 21\\nfood -combining, 3 7\\nwith high -carb ohydrate fruits, 33\\nmaking from raw cream, 50\\nfor thirst & dry mouth, 35\\n\\ncabbage jui ce, 161, 177\\n\\n(^192) Volume Four t he Recipe for Living Without Disease\\nCaesar Meat-Dressing (recipe), 65\\nCajun Chicken (recipe), 96\\ncalcification from pasteurized dairy, 185\\ncampylobacter bacteria, 18, 179\\ncancer\\namong Eskimos, 166\\ndestructiveness of conventional treatments, 9–10\\nhealing rate on cooked diets, 161\\nhigh meat and, 148-149\\nmicrobial treatments for, 170, 172\\nmodern medicine and, 164\\nPrimal Diet case, 176\\nrate of, 8\\ncandida, 32, 152\\ncanning jars for blending, 46–47\\nCaraway Cottage Cheese (recipe), 59\\ncarbohydrates\\nacrylamides from cooking, 155\\nfundamentals, 31–32\\nnut formula for cravings, 33\\nrestriction of, 21–22\\nroot vegetable juices, 30\\ncarnivores and digestion, 152\\nCarpaccio (recipe), 90\\ncarrot juice, 30, 32\\nCassidy, Dr. Paul B., 185\\ncatfish, freshwater, 29\\ncauterization, 154\\nCenter for Disease Control (CDC), 168, 170, 178, 187-188\\nCeviche (recipe), 107\\nCeviche, Thai (recipe), 113\\nCheesecake, Miniature (recipe), 122–123\\nCheesecake (recipe), 120–121\\ncheesecloth, 50\\ncheeses, raw\\nfood-combining, 36–37\\nwith high-carbohydrate fruits, 33\\nCheesy Chicken (recipe), 97\\nCheesy Spiced Paste (recipe), 65\\nchemicals\\nadded to foods, 157\\npollution from, 167\\nchemotherapy, destructive nature of, 10, 163\\nchicken, raw\\nhigh, 149\\nrecipes\\nCajun Chicken, 96\\nCheesy Chicken, 97\\nChicken & Tomato Soup, 113\\n\\nInde x 193\\n\\nChicken/Beef Mustard, 9 8\\nChicken Salad, 97\\nChicken Soup, 114\\nCream of Chicken Soup, 114\\nFrench Chicken, 98\\nMacaroni & Cheese-Tasting Chicken, 99\\nParmesan Chicken, 100\\nSalsa Chicken, 101\\nSexy Chicken, 102\\nTahitian Chicken, 10 2\\nchildren, raw milk for, 181–185\\nchol esterol\\nin eggs, 170\\nmyths regarding, 158\\nchronic fatig ue syndrome, 29, 176\\nCi pro (antibiotic), 175\\nclay, sun-dried\\neating with high meat, 148\\nin juice, 31\\nin toothp aste recipe, 148\\ncoconut cream\\nfood -combining, 36-3 7\\nwith high -carb ohydrate fruits, 33\\nfor lubrication and soothing, 34\\ntechniqu e for making, 49–50\\nCoconut Cream & Fruit (recipe), 123\\ncoconut oil, 34\\nCoffee Substitute (recipe), 57\\nCold, Flu, Severe Pain (formula), 147\\ncolic, 181\\nconstipation, 148-1 50\\nChronic Constip ation Formula, 1 48\\nHigh Meat Recipe, 149\\npasteurized milk and, 182\\nra w butter and, 24\\nTemporary Constip atio n Formula, 150\\ncooked & processed foods\\nanimal s, effect on, 157\\nbiological degeneratio n from, 180\\nde struction of nutrients, 153–158\\nin development of diseases, 11 , 166–168\\ndigestio n of, 15–16\\neffect on taste & flavor, 25\\npr oduc tion of toxins, 154–156\\ncooking, early development of, 15\\ncorporate interests, 178-179\\ncottage cheese recipes, 59–60\\nCaraway Cottage Cheese, 59\\nSour Cottage Cheese, 60\\n\\n(^194) Volume Four t he Recipe for Living Without Disease\\nSweet Cottage Cheese, 60\\nCream of Chicken Soup (recipe), 114\\ncream, raw\\nfood-combining, 36\\nwith high-carbohydrate fruits, 33\\nmaking raw butter from, 50\\nfor thirst & dry mouth, 35\\nwhipping technique, 51\\nCream Sickles, 133\\nCreamy Cheese Pepper Sauce (recipe), 66\\nCrewe, Dr. J.E., 184\\nCrohn's disease, 160-161\\nCustard Aphrodisiac (recipe), 124\\n\\ndead cells, 22-23, 34\\ndeficiencies and disease, 11\\ndehydration, 15, 35, 174\\ndental decay, 31, 166, 169\\nDeodorant, Natural (formula), 147\\ndepression and high meat, 148–149\\ndessert (sweet meal) recipes, 120–140\\nCheesecake, 120–123\\nCoconut Cream & Fruit, 123\\nCustard Aphrodisiac, 124\\nFudge Parfait or Mint Fudge Parfait, 125\\nGingerbread Balls, 126\\nIce Cream.\", 'x 45-90 minutes later, drink a blended Moisturizing/\\nLubrication Formula, page 146.', '3 to 4 level tablespoons refrigerated fresh soft bee pollen\\n1 to 2 ounces no-salt-added raw cheese\\n1 MOISTURIZING/LUBRICATION FORMULA DRINK\\n\\nAdd pollen to the Moisturizing/Lubrication Formula above prior\\nto blenderizing, and then blenderize.', 'See fish/seafood recipes\\nmedicine, allopathic\\ndeaths and injuries from medications, 8\\nmodern, 164–165\\norigins of, 162–164\\ntreatments causing disease, 8–9\\nmercury\\nbioactive vs. toxic, 29\\ndamage caused by, 175\\n\\nInde x 201\\n\\nand heavy metals removal (recipe), 56\\nmetabolism\\nfast, recommended eatin g schedule, 4 1\\nslow, recommended eatin g schedule, 40\\nmetals, heavy (r emoval formula), 56\\nMexican Sour Cream Sauce (recipe), 71\\nmicrobes\\nas disease eliminators, 170–174\\nlo w levels f rom drugs & chemicals, 188\\nmicrowave packs, 150\\nmilk\\npasteurized, 181–185\\nproducts, optimum temperatures for, 27\\nraw\\nbenefits for infants & children, 181–185\\nda ily intake of, 39\\nfood -combining & digestio n of, 36\\ninstead of water, 35\\nsafety of in scientific studies, 169\\nstudi es supporting health benefits of, 181–183\\nas therapeutic treatment, 184–185\\nMilkshake (recipe), 57\\nMillstone , Erik, 168\\nmineral absorption & utilization, 160\\nMint Chocolate Substitute (recipe), 127\\nMoisturizing/Lubrication Drink\\nin daily eating program, 40–41\\nrecipe for, 146\\nmolds, as detoxifyin g agents, 23\\nMonroe, Jon, 171\\nMornay Sauce (recipe), 72\\nMorris, Dr. Don, 172\\nMousseleine Sauce (recipe), 72\\nMushroom Cream Cheese Sauce (recipe), 73\\nMushroom Cream Sauce (recipe), 73\\nmustard, 44\\nMustard (recipes), 74–75\\n\\nna ps, 24\\nnativ e diets\\ncarbohydrates & Native Americans, 31\\nEskimo longevity, 165\\nmicrobes as medicine in, 171–172\\nraw fat consumption in, 28\\nnervous system\\nfruit and, 33\\nnerve-tissue regeneratio n, 2 9\\n\\n(^202) Volume Four t he Recipe for Living Without Disease\\nnut formula for neurological detoxification, 33\\nnutrients\\neffect of cooking on, 153–157\\nprocessing effects on, 157–158\\nnuts and seeds\\nactions and uses for, 33\\nas carbohydrates, 31\\nfood-combining, 36\\nNut & Spice Sauce (recipe), 75\\nnut formula\\nNut Butter/Nut Formula (recipe), 117\\nwhen and how to use, 33\\nNuts Over Meat (recipe), 94\\n\\noils\\ndrying effect of, 44\\nfundamentals, 34\\nolive, optimum temperature for, 26\\nomnivores, 153\\nonions, 44\\noptimal diet regimens, 39–41\\noptimal health, attaining, 23\\nOptimal Ways of Living, 189\\nOrange-Glazed Duck (recipe), 99\\nOrange Smoothie (recipe), 58\\norganic produce, 30\\norgans & glands (power drink recipes), 103–104\\nosteoporosis, 31, 152, 158, 166, 169, 181\\novaries, raw (recipe), 104\\nover-eating, 25\\nOyster Sauce & Pasta (recipe), 108\\nOysters Over Cheese (recipe), 109\\n\\nPain Remedy (formula), 146, 147\\npancreas, 22\\nparasites\\nas detoxifying agents, 23, 173-174\\nhealth benefits of, 173–174\\nParmesan Chicken (recipe), 100\\nPasta Substitute (recipe), 118\\nPasteur, Louis, 162–164\\npasteurization\\nof foods, 154, 157, 177, 179\\nof milk, 181–185\\n\\nInde x 203\\n\\npathogens\\ncause or result of disease, 19\\nas disease eliminators, 170–172\\nfa lse concern over, 177–178\\ntrue definition of, 1 79\\nwar on, 169–17 0\\nPecan Fudge (recipe), 128\\nPepita Gravy (recipe), 76\\npe sticides, 167–168\\npharmaceutical industry, 165, 187–188\\nPickles & Pickli ng recipes, 142– 144\\nDill Pickles, 14 3\\nPickled Ginger, 142\\nPickled Peppers (Pimentos), 14 3\\nSweet Pickles, 144\\npie recipes, 134–1 41\\nAmbrosia Coconut Cream Pie, 134–13 5\\nAmbrosia Cream Pie, 136–137\\nBanana Cream Pie, 138–139\\nPumpkin Pie, 140–141\\nPineapple Ice Cream (recipe), 133\\nplaque removal (formula), 55\\npollution, e nvironmental, 11, 19, 167–168, 180\\nPolynesian Ging er Sauce (recipe), 105\\npotato juice, 30\\nPottenger, Dr. Francis, 23, 1 65 –166, 181-183\\nPower Drinks\\nfor chronic fatigue, 29\\nrecipes (organs & glands), 103–104\\nLiver Booster, 1 03–104\\nThe Power Drink, 104\\nPrice, Dr. Weston, 182\\nPrimal Diet Web site, 14\\nPrimal Facial Body Care Cream (formula), 145\\nprocessed-food industry, 17, 32\\nprocessed foods.', 'When I give it to men, I call it the\\nLubrication Formula.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.08319902420043945s\n",
      "\n",
      "Query: What is the diet routine?\n",
      "Expected Sentence: General Daily Eating Schedule Recommendations #1 \n",
      "Retrieved Sentences: ['G: What?', 'What?”', '“What?”', 'What?”', 'What....?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.066680908203125s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the best juicer\n",
      "Expected Sentence: Place the coconut meat slices or ground coconut in a juicer that separates cream from pulp, such as a GreenStar juicer, Champion, or Norwalk.\n",
      "Retrieved Sentences: ['A: Good.', 'Good.', 'Good.', '“Good.', 'Good.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.06289291381835938s\n",
      "\n",
      "Query: Vegetable juice recipe\n",
      "Expected Sentence: I'm going to recommend that you have 10% carrot juice, 80% celery, and 10% parsley\n",
      "Retrieved Sentences: ['Before or after vegetable juice.', \"See nuts and seeds\\nSexy Chicken (recipe), 102\\nShrimp Passion (r ecipe), 110\\nskin care, 145–146\\nsleep and healing, 24\\nsolvents\\nfor cleansing, 31\\nstored fats as, 38\\nsoup recipes, 113–116\\nChicken, 114\\nChicken & Tomato, 113\\nCr eam of Chicken, 114\\nGrandma's Tomato, 115\\nLentil, 115\\nSplit Pea, 11 6\\nSour Cottage Cheese (recipe), 60\\nSour Cream Quick (recipe), 77\\nSour Cream (recipe), 76\\nSouth African Chipolata (recipe), 12 9\\nSouth African Frikkadel Glaze (recipe), 77\\nSpice Paste (recipe), 78\\nSpiced Butter or Oil (recipe), 79\\nSpiced Salmon (recipe), 111\\nSpiced Sashimi (recipe), 111\\nspices, 44–45\\nSpicy Afri can Paste for Fish (recipe), 106\\nSpicy African Paste (recipe), 80\\nSpicy Thai Sauce (recipe), 81\\nSplit Pea Soup (recipe), 116\\nsprouted grains, 152\\nstarch, raw, 117 –11 9\\nNut Butter/Nut Formul a, 11 7\\nPasta Substitute , 118\\nReminiscent of Mexican Chips, 1 18\\nReminiscent of Refried Beans, 119\\nSteak Tartare (recipe), 95\\nStefansson, Vilhjalmur , 166, 18 3\\nsturgeon, freshwater, 29\\nsunscreen formula, 145\\nsupplements\\nargument against consuming, 159–161\\nnegative influence of, 13\\n\\n(^210) Volume Four t he Recipe for Living Without Disease\\nSweet Cottage Cheese (recipe), 60\\nSweet Pickles (recipe), 144\\nswordfish\\nmercury and, 29\\nSwordfish Sashimi (recipe), 112\\n\\nTahitian Chicken (recipe), 102\\nTahitian Fish (recipe), 112\\nTango Meat Sauce (recipe), 82\\nTartar Coconut Cream Sauce (recipe), 83\\nTartar Sauce (recipe), 83\\ntemper, control of, 117\\ntemperature ranges\\ndestruction of nutrients, 153-154\\noptimum food, 26–27\\ntestes, raw (recipe), 104\\nThai Ceviche (recipe), 113\\nthimerosal, 175\\nthirst, 35\\nThousand Island Meat-Dressing (recipes), 84\\nThroat Lozenges, Lemon (formula), 147\\nthyroid gland, raw, 104\\nTomato Cream Cheese Sauce (recipe), 85\\nTomato Sauce (recipe), 85\\ntomatoes\\nfood-combining as fruit, 36\\nwith raw fat for dryness and thirst, 35\\ntooth decay, 21, 169, 182\\nToothpaste (formula), 148\\ntoxicity\\ncarcinogens & cooking, 17\\nas cause of disease, 11–12\\nremoving deep-tissue, 38–43\\ntoxic salts removal (formula), 54\\ntraveling recommendations, 42–43\\ntuberculosis, pulmonary, 184\\ntumors, 34, 171, 176\\nTurkey Pâté (recipe), 103\\n\\nVan Wagtendork, Willem J., 185\\nVan Winkle, Dr. Elnora, 179\\nvegetables & vegetable juices.\", 'And a quart of vegetable juice.', 'and\\nvegetables.', 'Several juice recipes appear on pages 54-56.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07671904563903809s\n",
      "\n",
      "Query: What to eat while travelling?\n",
      "Expected Sentence: I was wondering what do you do if you’re travelling\n",
      "Retrieved Sentences: ['What....?', 'What?”', '“What?”', 'What?”', 'What’s To Discuss?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07693219184875488s\n",
      "\n",
      "Query: How fast to sip liquids?\n",
      "Expected Sentence: But, I never take more than two oz. of cold milk at a time.\n",
      "Retrieved Sentences: ['I seemed liquid now.', 'If continual cramps occur, it would be best to sip milkshake all day\\nlong.', 'I’ll sip it.', 'She sipped it.', 'Q: So what do you...its just hard to sip stuff.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07606005668640137s\n",
      "\n",
      "Query: What to eat while travelling?\n",
      "Expected Sentence: For every 14 days of travel, I take 21⁄2 pounds of no-salt-added raw cheeses, 1 quart of unheated honey and 3 pounds of unsalted raw butter. \n",
      "\n",
      "Retrieved Sentences: ['What....?', 'What?”', '“What?”', 'What?”', 'What’s To Discuss?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.0771639347076416s\n",
      "\n",
      "Query: What are cheese trains?\n",
      "Expected Sentence: It will go through like a train and just pick it up\n",
      "Retrieved Sentences: ['G: What is that?', 'What is that?', 'What is that?', 'What?”', 'Q: What?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07603979110717773s\n",
      "\n",
      "Query: How much cheese in a cheese train?\n",
      "Expected Sentence: I said ‘When you get up in the morning, you eat a tablespoon or 2 tablespoons.\n",
      "Retrieved Sentences: ['I would eat\\ncheese and an egg, cheese in a milkshake, or cheese and meat.', 'And how much meat?', '... enough cheese?', 'Q: How?', 'How?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.06731390953063965s\n",
      "\n",
      "Query: What does cheese do?\n",
      "Expected Sentence: First thing in the morning absorb all the poisons that have dumped into the stomach and intestines.\n",
      "Retrieved Sentences: ['Q: What?', 'What....?', 'What do they do?', 'What do they do?', 'Cheese?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07654500007629395s\n",
      "\n",
      "Query: Timing cheese consumption\n",
      "Expected Sentence: Do not have fruit or anything high in carbohydrates with those cheeses and eat cheese 10 minutes before you eat any meal \n",
      "Retrieved Sentences: ['Cheese, cheese, cheese, cheese, cheese – absorb all that\\nstuff.', 'lots of cheese.', 'And the cheese.', 'Farmer’s cheese?', 'Long time.']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07682490348815918s\n",
      "\n",
      "Query: Maximum temperature for hot baths\n",
      "Expected Sentence: 110 degrees is when the highest point you will get the skin without damaging the bacteria and the enzymes in the skin.\n",
      "Retrieved Sentences: [\"Q: I didn't understand but...\\n\\nA: That was for hot, it's for making it really hot.\", 'People who do not have spleens should ease into hot baths and drink more fluids immediately prior to entering a hot bath.', 'When I take one of those hot baths.', 'do you recommend those\\nhot baths?', 'G: What temperature?']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.08051204681396484s\n",
      "\n",
      "Query: Why take hot baths\n",
      "Expected Sentence:  You have got to get those plastic molecules out of your system, out of your way\n",
      "Retrieved Sentences: ['Sleep with hot water bottles if you can’t take hot\\nbaths or only have a shower.', 'Why? ...', 'Why? ...', 'Why?', 'Why? ...']\n",
      "Is Relevant: False\n",
      "Query Precision: 0, Query Recall: 0\n",
      "Query Execution Time: 0.07669711112976074s\n",
      "\n",
      "Average Precision: 0.0\n",
      "Average Recall: 0.0\n",
      "Average F1-Score: 0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_execution_time = 0\n",
    "\n",
    "# Check if the expected sentence is among the retrieved sentences\n",
    "def is_relevant(retrieved_docs, relevant_doc):\n",
    "    return relevant_doc in retrieved_docs\n",
    "\n",
    "total_precision = total_recall = 0\n",
    "num_queries = len(labelled_queries_df)\n",
    "\n",
    "for index, row in labelled_queries_df.iterrows():\n",
    "    query = row['Relevant Query']\n",
    "    expected_sentence = row['Sentence']\n",
    "\n",
    "    start_time = time.time()  \n",
    "    top_docs, _ = search(query, vectorizer, tfidf_test_matrix, test_set_df)\n",
    "    execution_time = time.time() - start_time  \n",
    "    total_execution_time += execution_time\n",
    "    \n",
    "    retrieved_sentences = top_docs['sentence'].tolist()\n",
    "\n",
    "    relevant = is_relevant(retrieved_sentences, expected_sentence)\n",
    "\n",
    "    precision = 1 if relevant else 0\n",
    "    recall = 1 if relevant else 0\n",
    "\n",
    "    total_precision += precision\n",
    "    total_recall += recall\n",
    "    \n",
    "    # debugging\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Expected Sentence: {expected_sentence}\")\n",
    "    print(f\"Retrieved Sentences: {retrieved_sentences[:5]}\")\n",
    "    print(f\"Is Relevant: {relevant}\")\n",
    "    print(f\"Query Precision: {precision}, Query Recall: {recall}\")\n",
    "    print(f\"Query Execution Time: {execution_time}s\\n\")\n",
    "\n",
    "average_precision = total_precision / num_queries\n",
    "average_recall = total_recall / num_queries\n",
    "average_f1_score = 2 * (average_precision * average_recall) / (average_precision + average_recall) if (average_precision + average_recall) != 0 else 0\n",
    "\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(f\"Average Recall: {average_recall}\")\n",
    "print(f\"Average F1-Score: {average_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61276110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date Dataset Version  Max DF  Min DF Ngram Range  \\\n",
      "0 2023-11-20 14:04:39.454885              v2     0.4     0.0      (1, 4)   \n",
      "\n",
      "   Precision  Recall  F1-Score  Total Execution Time Comments  \n",
      "0        0.0     0.0         0              1.109702      ...  \n"
     ]
    }
   ],
   "source": [
    "PARAM_DIR = Path.cwd() / \"aajonus_hyperparameter_table\"\n",
    "PARAM_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_next_table_index(directory):\n",
    "    existing_files = [f for f in directory.iterdir() if f.is_file()]\n",
    "    return len(existing_files) + 1\n",
    "\n",
    "table_index = get_next_table_index(PARAM_DIR)\n",
    "full_param_path = PARAM_DIR / f\"hyperparameter_table_{table_index}.csv\"\n",
    "\n",
    "# Add results to the hyperparameter table\n",
    "hyperparameter_results = pd.DataFrame(columns=[\n",
    "    \"Date\", \n",
    "    \"Dataset Version\", \n",
    "    \"Max DF\", \n",
    "    \"Min DF\", \n",
    "    \"Ngram Range\", \n",
    "    \"Precision\", \n",
    "    \"Recall\", \n",
    "    \"F1-Score\", \n",
    "    \"Total Execution Time\", \n",
    "    \"Comments\", \n",
    "])\n",
    "\n",
    "hyperparameter_results.loc[len(hyperparameter_results)] = [\n",
    "    pd.Timestamp('now'), \n",
    "    \"v2\",  \n",
    "    max_df, \n",
    "    min_df, \n",
    "    ngram_range, \n",
    "    average_precision,  \n",
    "    average_recall,     \n",
    "    average_f1_score,   \n",
    "    total_execution_time, \n",
    "    \"Comment\",  \n",
    "]\n",
    "\n",
    "hyperparameter_results.to_csv(full_param_path, index=False)\n",
    "\n",
    "print(hyperparameter_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5b34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
