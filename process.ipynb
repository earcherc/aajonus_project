{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e05e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x16042d510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needles_Of_Disease_and_Death_Continue_In_The_Name_Of_Saving_Children.txt\n",
      "Diarrhea-based_Detoxification_Hotel_By_Medical_Doctors.txt\n",
      "The_FDA_Approved_5_Viruses_for_Food_Treatment.txt\n",
      "Genius_Children.txt\n",
      "Dr._Stanley_S._Bass_Interview.txt\n",
      "Q&A_Of_September_13,_2009.txt\n",
      "Causes_For_Most_Intestinal_Disease.txt\n",
      "Are_Raw_Miso_And_Shoyu_Healthy_Sauces?.txt\n",
      "Safe_Cutting_Boards.txt\n",
      "Multiple_Lacerations_Healed_Without_Medical_Help.txt\n",
      "Cholesterol,_LDL_and_HDL.txt\n",
      "Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt\n",
      "Can_We_Preserve_Raw_Chicken_In_Vinegar_Or_Lemon_Juice?.txt\n",
      "Abrasions,_Fractures_and_Breaks.txt\n",
      "Is_Raw_Chocolate_Made_From_Whole_Raw_Cocoa_Beans_Addictive_Or_Harmful?.txt\n",
      "What_Is_Constipation_And_How_Do_We_Resolve_It?.txt\n",
      "Our_Ubiquitous_Microbial_Friends.txt\n",
      "Quinton.txt\n",
      "Q&A_Of_December_14,_2008.txt\n",
      "Q&A_Of_October_14,_2012.txt\n",
      "My_Survival_Kit.txt\n",
      "Medical_Propaganda_about_Inflammatory_Breast_Cancer.txt\n",
      "How_Are_Nutrients_Delivered_To_Our_Cells?.txt\n",
      "Q&A_Of_August_24,_2008.txt\n",
      "Vaccines_Ruin_Your_Health.txt\n",
      "Frozen_Pastured_Meat_VS_Fresh_Supermarket_Meat.txt\n",
      "Long-term_Damage_From_Abduction_and_Forced_Injections.txt\n",
      "Q&A_Of_May_26,_2013.txt\n",
      "FLU_-_Viral_Tools_Improve_Health.txt\n",
      "Email_from_Aajonus_Summer_2012.txt\n",
      "Dental_Hygiene,_Causes_of_Decay_and_Reversal,_and_Re-enamelization.txt\n",
      "Bad_And_Good_Parasites,_And_Malaria?.txt\n",
      "View_on_Medical_Establishment.txt\n",
      "Formula_for_a_Healthy_Baby.txt\n",
      "Top_Aussie_Doctor_Says_Pick_Your_Nose_And_Eat_it.txt\n",
      "Supplements.txt\n",
      "Primal_Diet_Workshop_(Part_1).txt\n",
      "Raw_Dairy_Is_Healthy_But_Illegal.txt\n",
      "Does_Rabies_Exist?.txt\n",
      "Q&A_Of_May_2,_2004.txt\n",
      "Abduction_and_Injections.txt\n",
      "Is_Raw_Milk_Always_Beneficial_Even_With_Much_Bacteria?.txt\n",
      "Q&A_Of_September_10,_2006.txt\n",
      "How_Long_Does_It_Take_To_Understand_The_Primal_Diet(TM)?.txt\n",
      "Rawesome_Trial_Outcome.txt\n",
      "Q&A_Of_June_29,_2008.txt\n",
      "Whole_Foods_Markets,_Inc._Friend_To_Better_Health_Or_Foe?.txt\n",
      "Q&A_Of_December_17,_2006.txt\n",
      "What_Should_We_Consider_For_Health_When_Buying_A_New_Car?.txt\n",
      "Q&A_Of_January_24,_1999.txt\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "5  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "6  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "7  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "8  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "9  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \n",
      "0  On Halloween, I received the most alarming ter...  \n",
      "1  I received it\\nin a letter from Care2 organiza...  \n",
      "2  Most of us have\\nnever witnessed the crippling...  \n",
      "3  Polio is still endemic in three of the world's...  \n",
      "4  This is the scary truth: levels of polio are a...  \n",
      "5  Eradication is\\nwithin reach, but we need your...  \n",
      "6  In fact, if we don't end polio now,\\nit could ...  \n",
      "7  The United Nations Foundation's Shot@Life camp...  \n",
      "8                                We're almost there!  \n",
      "9  It costs less than $1 to vaccinate a child aga...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"aajonus_data\"\n",
    "\n",
    "# Initialize a list to store your data\n",
    "data = []\n",
    "\n",
    "# Initialize a list to store your data\n",
    "data = []\n",
    "\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    # Only process the first 20 files\n",
    "    if file_count >= 50:\n",
    "        break\n",
    "\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_count += 1\n",
    "\n",
    "        print(filename)\n",
    "\n",
    "        # Create the full filepath\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "            # Use spaCy to tokenize the content into sentences\n",
    "            doc = nlp(content)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents]\n",
    "            # Append each sentence to your data list, along with the filename\n",
    "            for sentence in sentences:\n",
    "                data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the first 10 rows of the DataFrame\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95247a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_lemmatize(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0278fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "5  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "6  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "7  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "8  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "9  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \\\n",
      "0  On Halloween, I received the most alarming ter...   \n",
      "1  I received it\\nin a letter from Care2 organiza...   \n",
      "2  Most of us have\\nnever witnessed the crippling...   \n",
      "3  Polio is still endemic in three of the world's...   \n",
      "4  This is the scary truth: levels of polio are a...   \n",
      "5  Eradication is\\nwithin reach, but we need your...   \n",
      "6  In fact, if we don't end polio now,\\nit could ...   \n",
      "7  The United Nations Foundation's Shot@Life camp...   \n",
      "8                                We're almost there!   \n",
      "9  It costs less than $1 to vaccinate a child aga...   \n",
      "\n",
      "                            expanded_lemmatized_text  \n",
      "0  on halloween , I receive the most alarming ter...  \n",
      "1  I receive it \\n in a letter from care2 organiz...  \n",
      "2  most of we have \\n never witness the crippling...  \n",
      "3  polio be still endemic in three of the world '...  \n",
      "4  this be the scary truth : level of polio be at...  \n",
      "5  eradication be \\n within reach , but we need y...  \n",
      "6  in fact , if we do not end polio now , \\n it c...  \n",
      "7  the united nations foundation 's shot@life cam...  \n",
      "8                               we be almost there !  \n",
      "9  it cost less than $ 1 to vaccinate a child aga...  \n"
     ]
    }
   ],
   "source": [
    "df['expanded_lemmatized_text'] = df['sentence'].apply(spacy_lemmatize)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f929233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text\n",
    "tfidf_matrix = vectorizer.fit_transform(df['expanded_lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "937a4579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/ethancavill/Documents/notebooks/aajonus_joblibs/tfidf_matrix.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib_dir = Path.cwd() / \"aajonus_joblibs\"\n",
    "joblib_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Define the full path for the vectorizer and matrix joblib files\n",
    "vectorizer_path = joblib_dir / 'tfidf_vectorizer.joblib'\n",
    "matrix_path = joblib_dir / 'tfidf_matrix.joblib'\n",
    "\n",
    "# Save the vectorizer and matrix to disk in the specified directory\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "joblib.dump(tfidf_matrix, matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  \\\n",
      "13688                        Q&A_Of_May_26,_2013.txt   \n",
      "5294   Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "5770   Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "9111                     Q&A_Of_October_14,_2012.txt   \n",
      "14971              Primal_Diet_Workshop_(Part_1).txt   \n",
      "16618                  Q&A_Of_September_10,_2006.txt   \n",
      "8356                    Q&A_Of_December_14,_2008.txt   \n",
      "9529                     Q&A_Of_October_14,_2012.txt   \n",
      "3691   Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "5293   Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt   \n",
      "\n",
      "                                                sentence  \\\n",
      "13688  I've got people who only do high meat,\\nonly d...   \n",
      "5294   High meats, I would say, not for about 2.5 years.   \n",
      "5770        I haven't really done much with high\\nmeats.   \n",
      "9111                 G: How do you make high meat juice?   \n",
      "14971  Actually, Eskimos gave me the introduction, bu...   \n",
      "16618  (49) High Meat Vs Fecal Matter\\n[2006],[Aajonu...   \n",
      "8356                    Q: Can high meat do any of that?   \n",
      "9529                                           You high?   \n",
      "3691                                       Or high meat.   \n",
      "5293                                     And high meats?   \n",
      "\n",
      "                                expanded_lemmatized_text  \n",
      "13688  I 've get people who only do high meat , \\n on...  \n",
      "5294   high meat , I would say , not for about 2.5 ye...  \n",
      "5770       I have not really do much with high \\n meat .  \n",
      "9111               g : how do you make high meat juice ?  \n",
      "14971  actually , eskimos give I the introduction , b...  \n",
      "16618  ( 49 ) high meat vs fecal matter \\n [ 2006],[a...  \n",
      "8356                  q : can high meat do any of that ?  \n",
      "9529                                          you high ?  \n",
      "3691                                      or high meat .  \n",
      "5293                                     and high meat ?  \n",
      "[0.57509707 0.57555018 0.57637649 0.5930304  0.61478865 0.63602509\n",
      " 0.66363413 0.69671752 0.86187192 0.94996897]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, vectorizer, tfidf_matrix, df):\n",
    "    # Preprocess the query\n",
    "    preprocessed_query = spacy_lemmatize(query)\n",
    "    \n",
    "    # Vectorize the query\n",
    "    query_vector = vectorizer.transform([preprocessed_query])\n",
    "    \n",
    "    # Compute cosine similarity between the query and the documents\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    \n",
    "    # Get the top 5 most similar document indices\n",
    "    top_indices = similarities.argsort()[0][-10:]\n",
    "    \n",
    "    # Return the most similar documents and their similarity scores\n",
    "    return df.iloc[top_indices], similarities[0][top_indices]\n",
    "\n",
    "# Test the search with an example query\n",
    "example_query = \"high meat\"\n",
    "top_docs, scores = search(example_query, vectorizer, tfidf_matrix, df)\n",
    "print(top_docs)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba6b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61276110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5b34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
