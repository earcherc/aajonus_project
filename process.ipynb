{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6084335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ethancavill/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e05e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x164e69f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needles_Of_Disease_and_Death_Continue_In_The_Name_Of_Saving_Children.txt\n",
      "Diarrhea-based_Detoxification_Hotel_By_Medical_Doctors.txt\n",
      "The_FDA_Approved_5_Viruses_for_Food_Treatment.txt\n",
      "Genius_Children.txt\n",
      "Dr._Stanley_S._Bass_Interview.txt\n",
      "Q&A_Of_September_13,_2009.txt\n",
      "Causes_For_Most_Intestinal_Disease.txt\n",
      "Are_Raw_Miso_And_Shoyu_Healthy_Sauces?.txt\n",
      "Safe_Cutting_Boards.txt\n",
      "Multiple_Lacerations_Healed_Without_Medical_Help.txt\n",
      "Cholesterol,_LDL_and_HDL.txt\n",
      "Primal_Diet_Workshop_+_Q&A_Of_May_6,_2000.txt\n",
      "Can_We_Preserve_Raw_Chicken_In_Vinegar_Or_Lemon_Juice?.txt\n",
      "Abrasions,_Fractures_and_Breaks.txt\n",
      "Is_Raw_Chocolate_Made_From_Whole_Raw_Cocoa_Beans_Addictive_Or_Harmful?.txt\n",
      "What_Is_Constipation_And_How_Do_We_Resolve_It?.txt\n",
      "Our_Ubiquitous_Microbial_Friends.txt\n",
      "Quinton.txt\n",
      "Q&A_Of_December_14,_2008.txt\n",
      "Q&A_Of_October_14,_2012.txt\n",
      "My_Survival_Kit.txt\n",
      "Medical_Propaganda_about_Inflammatory_Breast_Cancer.txt\n",
      "How_Are_Nutrients_Delivered_To_Our_Cells?.txt\n",
      "Q&A_Of_August_24,_2008.txt\n",
      "Vaccines_Ruin_Your_Health.txt\n",
      "Frozen_Pastured_Meat_VS_Fresh_Supermarket_Meat.txt\n",
      "Long-term_Damage_From_Abduction_and_Forced_Injections.txt\n",
      "Q&A_Of_May_26,_2013.txt\n",
      "FLU_-_Viral_Tools_Improve_Health.txt\n",
      "Email_from_Aajonus_Summer_2012.txt\n",
      "Dental_Hygiene,_Causes_of_Decay_and_Reversal,_and_Re-enamelization.txt\n",
      "Bad_And_Good_Parasites,_And_Malaria?.txt\n",
      "View_on_Medical_Establishment.txt\n",
      "Formula_for_a_Healthy_Baby.txt\n",
      "Top_Aussie_Doctor_Says_Pick_Your_Nose_And_Eat_it.txt\n",
      "Supplements.txt\n",
      "Primal_Diet_Workshop_(Part_1).txt\n",
      "Raw_Dairy_Is_Healthy_But_Illegal.txt\n",
      "Does_Rabies_Exist?.txt\n",
      "Q&A_Of_May_2,_2004.txt\n",
      "Abduction_and_Injections.txt\n",
      "Is_Raw_Milk_Always_Beneficial_Even_With_Much_Bacteria?.txt\n",
      "Q&A_Of_September_10,_2006.txt\n",
      "How_Long_Does_It_Take_To_Understand_The_Primal_Diet(TM)?.txt\n",
      "Rawesome_Trial_Outcome.txt\n",
      "Q&A_Of_June_29,_2008.txt\n",
      "Whole_Foods_Markets,_Inc._Friend_To_Better_Health_Or_Foe?.txt\n",
      "Q&A_Of_December_17,_2006.txt\n",
      "What_Should_We_Consider_For_Health_When_Buying_A_New_Car?.txt\n",
      "Q&A_Of_January_24,_1999.txt\n",
      "Lobbying_in_Washington,_DC_for_Raw_Milk.txt\n",
      "What_Do_We_Do_About_Emerging_Plagues?.txt\n",
      "Is_The_Science_of_Viruses_Real?.txt\n",
      "Q&A_Of_May_20,_2012.txt\n",
      "Chemtrails.txt\n",
      "Q&A_Of_February_22,_2009.txt\n",
      "How_Can_EMFs_Cause_Death_Prematurely?.txt\n",
      "A_New_Theory_Of_Disease.txt\n",
      "Bruises,_Injuries_and_Pain_-_Do_We_Apply_Ice_Or_Heat?.txt\n",
      "Malaria.txt\n",
      "My_Research_And_Experiments_Questioned.txt\n",
      "Q&A_Of_February_18,_2007.txt\n",
      "TB_Testing_for_Teachers_and_Health_Practitioners.txt\n",
      "Primal_Diet_Is_Not_A_Stagnant_Diet.txt\n",
      "Protecting_From_Medical_Treatments_in_Emergencies.txt\n",
      "Care_To_Have_A_Piss_Of_A_Drink_With_Me?.txt\n",
      "Q&A_Of_February_20,_2005.txt\n",
      "Natural_Toys,_Oh,_My!.txt\n",
      "Plastic_consumption_might_lead_to_sexual_disorders.txt\n",
      "Q&A_Of_June_3,_2007.txt\n",
      "Q&A_Of_January_29,_2006.txt\n",
      "At_What_Age_Is_Death_Inevitable?.txt\n",
      "What_Is_Nutrient_Value_Of_Dehydrated_foods?.txt\n",
      "Loss_Of_My_BIOHAZARDS_Research.txt\n",
      "Interview_by_Mina_Olen,_Health_magazine_in_Finland..txt\n",
      "Q&A_Of_January_22,_2000.txt\n",
      "Q&A_Of_May_27,_2012.txt\n",
      "Long-term_Delayed_Detoxification;_58_Years_Later.txt\n",
      "A_Wonderful_Ruling_in_Communist_China_That_We_Should_Adopt.txt\n",
      "Q&A_Of_August_20,_2006.txt\n",
      "Will_We_Continually_Pay_for_Medical_Mass_Poisoning_of_Humanity?.txt\n",
      "More_Clarity_On_Food-borne_Bacterial_Contamination.txt\n",
      "Charlie_Donham_Interview.txt\n",
      "London_Times_Interview.txt\n",
      "Q&A_Of_April_14,_2002.txt\n",
      "Do_We_Believe_Medical_Studies?.txt\n",
      "Frozen_Fish_VS_Frozen_Land_Animals.txt\n",
      "Who_Has_The_Right_To_Institutionalize_Me?.txt\n",
      "Paul_Andrews_Interview.txt\n",
      "Mercury_In_Fish;_Do_We_Absorb_It?.txt\n",
      "Benefits_of_Raw_Eggs.txt\n",
      "Q&A_Of_March_11,_2012.txt\n",
      "Study_Of_Thyroid_Cancer.txt\n",
      "Q&A_Of_September_19,_2000_(Rare).txt\n",
      "Vaccines;_Nice_Shots_Or_Not.txt\n",
      "What_Are_Drugs_And_Supplements?.txt\n",
      "What_Water_Should_I_Buy?.txt\n",
      "Vaccines,_All_Harmful_Or_Some_Beneficial?.txt\n",
      "Are_Citizens_Being_Attacked_By_Their_Governments?.txt\n",
      "What_Is_Our_Likelihood_Of_Developing_Cancer(s)?.txt\n",
      "Q&A_Of_June_10,_2007_&_September_9,_2007.txt\n",
      "Q&A_Of_September_9,_2007.txt\n",
      "Chemicals_Used_to_Protect_Food_From_Bacteria;_Harmful.txt\n",
      "Salt_And_Headaches.txt\n",
      "Human_Papillomavirus_(HPV)_Vaccine.txt\n",
      "Q&A_Of_May_24,_2009.txt\n",
      "Q&A_Of_November_17,_2000.txt\n",
      "Toxic_Chemicals_Out-Gassing_Into_Our_Homes.txt\n",
      "Corporate_&_Government_Tyranny_Behind_Disease.txt\n",
      "Q&A_Of_September_11,_2011.txt\n",
      "Q&A_Of_February_1,_2004.txt\n",
      "Q&A_Of_March_17,_2002.txt\n",
      "Iridology.txt\n",
      "Medications_Are_Toxic_And_Store_In_The_Tissue.txt\n",
      "H1N1_(Swine)_Flu_Epidemic,_Fact_or_Hoax?.txt\n",
      "Q&A_Of_May_30,_2010.txt\n",
      "How_Much_Bacteria_Are_We_Today?.txt\n",
      "LA_Times_Article_About_Raw-Foods.txt\n",
      "Superfoods.txt\n",
      "Why_Mercury_Is_Not_Absorbed_When_We_Eat_Raw_Fish_(Theory).txt\n",
      "Q&A_Of_April_17,_2005.txt\n",
      "Does_Raw_Milk_Do_A_Body_Good?.txt\n",
      "Q&A_Of_July_10,_2011.txt\n",
      "Q&A_Of_November_14,_2004.txt\n",
      "Exercise;_The_Good,_Bad_and_Beautiful.txt\n",
      "How_Much_Energy_Should_I_Expect_To_Experience?.txt\n",
      "Q&A_Of_August_17,_2003_(Full).txt\n",
      "What_Would_Happen_If_Aajonus_Ate_Some_Cooked_Meat?.txt\n",
      "About.txt\n",
      "Eating_Out,_Is_It_Safe?.txt\n",
      "Resolving_Early_Morning_Racing_Mind.txt\n",
      "Medical_Researchers_Proved_90%_Medical_Research_Is_False.txt\n",
      "Fermented_Vegetables;_The_Good,_Bad_and_Stinky.txt\n",
      "Q&A_Of_April_6,_2008.txt\n",
      "Q&A_Of_January_27,_2013.txt\n",
      "Gallbladder_Stones.txt\n",
      "Q&A_Of_July_20,_2008.txt\n",
      "Q&A_Of_May_29,_2011.txt\n",
      "Quality_or_Quantity?.txt\n",
      "Q&A_Of_May_21,_2000_(Rare).txt\n",
      "Q&A_Of_March_19,_2006.txt\n",
      "Grow_In_Height_After_Age_21.txt\n",
      "Q&A_Of_April_11,_2004.txt\n",
      "Pickled_Fish.txt\n",
      "The_Recipe_for_Living_Without_Disease.txt\n",
      "Q&A_Of_February_9,_2003.txt\n",
      "Severe_Back_Deterioration.txt\n",
      "How_To_Use_An_EMF_Meter.txt\n",
      "Interview_on_Talksport.net.txt\n",
      "Q&A_Of_November_27,_2005.txt\n",
      "Q&A_Of_January_9,_2011.txt\n",
      "Depression.txt\n",
      "Home-grown_Vegetables_Blamed_For_Disease.txt\n",
      "Ingredients_In_Injections_With_Hair_And_Iridology_Analyses.txt\n",
      "Q&A_Of_September_12,_2005.txt\n",
      "Dissolving_Plaque_from_Our_Circulatory_Systems.txt\n",
      "Why_Do_Most_Physicians_Refuse_Chemo-treatments?.txt\n",
      "How_Toxic_is_Our_Civilized_World?.txt\n",
      "Child_Cured_by_Mothers_Feces_-_Eat_Shit_and_Live!.txt\n",
      "Oysters_-_Special_Food_In_Our_Toxic_World.txt\n",
      "How_To_Remove_Fear_Of_Microbes.txt\n",
      "Q&A_Of_June_16,_2013.txt\n",
      "Q&A_Of_September_14,_2003.txt\n",
      "How_Do_Electromagnetic_Fields_Affect_Us?.txt\n",
      "Q&A_Of_September_12,_2004.txt\n",
      "What_Is_Nutrient_Value_Trace_Minerals?.txt\n",
      "Q&A_Of_February_25,_2007.txt\n",
      "Will_Big_Industries_Control_Us_via_Government?.txt\n",
      "Considering_Chemotherapy_As_An_Option_For_Cancer?.txt\n",
      "Q&A_Of_July_8,_2001.txt\n",
      "Q&A_Of_August_22,_2011.txt\n",
      "Blood_Types_For_Meat.txt\n",
      "Primal_Diet_Workshop_+_Q&A_Of_June_22,_2013.txt\n",
      "Baby_Food_For_Mothers_Who_Can't_Breastfeed.txt\n",
      "Q&A_Of_July_24,_2005.txt\n",
      "Bacteria,_Antibiotics,_Immune_System.txt\n",
      "Q&A_Of_March_26,_2000.txt\n",
      "Ball_and_Kerr_Jar_Lids,_Are_They_Plastic_Coated_and_Toxic_or_Not?.txt\n",
      "Q&A_Of_June_22,_2003.txt\n",
      "Does_Food_Affect_Behavior?.txt\n",
      "Q&A_Of_November_18,_2007.txt\n",
      "Q&A_Of_April_16,_2006.txt\n",
      "Friendly_Bacteria_Protect_Against_Type_1_Diabetes.txt\n",
      "Are_There_Aggressive_Treatments_For_Cancer?.txt\n",
      "Microbe_Food-Poisoning;_Fact_or_Fiction?.txt\n",
      "Primal_Diet_Workshop_(Part_2).txt\n",
      "What_Place_Do_Energy_Therapies_Take_In_Healing?.txt\n",
      "Chemical_Burns_Can_Be_Devastating_But_Managed_And_Healed.txt\n",
      "E.coli.txt\n",
      "Effects_of_Dietary_and_Environmental_Pollution_on_Children's_Sleep.txt\n",
      "Study_Shows_That_Chubby_People_Live_Longest.txt\n",
      "Q&A_Of_March_18,_2012.txt\n",
      "Oxalates.txt\n",
      "Quick_Alternative_Cure_For_Arthritis;_True_Or_False?.txt\n",
      "Is_It_True_You_Eat_Buckets_Of_Cow_Dung?.txt\n",
      "Can_We_Preserve_Raw_Fish_In_Oil?.txt\n",
      "100%_Of_Fresh-Water_Lakes_And_Streams_Are_Polluted_With_Mercury.txt\n",
      "Q&A_Of_May_7,_2006.txt\n",
      "Non-organic_Meat.txt\n",
      "Coronavirus.txt\n",
      "Repeated_Surgeries_Resulted_In_Thick_Scars.txt\n",
      "Q&A_Of_October_24,_2010.txt\n",
      "Question_And_Answers.txt\n",
      "Man_Eats_Live_Frogs_and_Rats_for_Health.txt\n",
      "Dangers_Of_Salt.txt\n",
      "Iron_On_The_Primal_Diet,_Is_It_A_Problem?.txt\n",
      "Q&A_Of_February_3,_2013.txt\n",
      "Q&A_Of_March_26,_2013.txt\n",
      "FDA_Approves_6_Viruses_As_Safe.txt\n",
      "Beneficial_Home_Baths.txt\n",
      "New_Source_Of_Stem_Cells_-_Mouse_Sperm.txt\n",
      "Athletes_And_Longevity_On_Primal_Diet.txt\n",
      "Q&A_Of_August_17,_2003.txt\n",
      "What_Role_Do_Genetics_and_Microbes_Play_In_Disease?.txt\n",
      "Benzene,_Cancer_and_Soft_Drinks_Connection.txt\n",
      "Primal_Diet_Workshop_(Part_3).txt\n",
      "High_Blood-Pressure.txt\n",
      "Q&A_Of_May_23,_2010.txt\n",
      "Rae_Bradbury_Interview_1.txt\n",
      "Is_It_Good_To_Donate_To_Charities_That_Feed_The_Poor_and_Starving?.txt\n",
      "Q&A_Of_April_4,_2010.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With_Mercury_Found_In_Wild_Animals,_Do_We_Need_To_Be_Extra_Careful?.txt\n",
      "Q&A_Of_September_26,_2010.txt\n",
      "Do_You_Buy_Chicken_While_Traveling?.txt\n",
      "Cancer_Convention_September_2000.txt\n",
      "Q&A_Of_November_7,_1999.txt\n",
      "Q&A_Of_November_26,_2006.txt\n",
      "How_Bad_Are_MRIs?.txt\n",
      "Arsenic_In_Poultry_Meat_And_Eggs.txt\n",
      "Joanne_Unleahsed_Interview.txt\n",
      "Declaring_Our_Rights_To_Our_Body.txt\n",
      "We_Want_To_Live.txt\n",
      "Soy_Toxicity_In_Poultry_Meat_And_Eggs.txt\n",
      "Hot_Tub_Therapy.txt\n",
      "Bacteria_and_Other_Microbes_Are_Responsible_for_Vibrant_Health.txt\n",
      "Gum_And_Tooth_Disease.txt\n",
      "Rae_Bradbury_Interview_2.txt\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \n",
      "0  On Halloween, I received the most alarming ter...  \n",
      "1  I received it\\nin a letter from Care2 organiza...  \n",
      "2  Most of us have\\nnever witnessed the crippling...  \n",
      "3  Polio is still endemic in three of the world's...  \n",
      "4  This is the scary truth: levels of polio are a...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"aajonus_data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"aajonus_saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files\n",
    "# If yes, then reinitialise these as dfs\n",
    "# If not, then create the dfs and save them in csv format for next run\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            # Create the filepath\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # Use spaCy to tokenize the content into sentences\n",
    "                doc = nlp(content)\n",
    "                sentences = [sent.text.strip() for sent in doc.sents]\n",
    "                # Append each sentence to your data list, along with the filename\n",
    "                for sentence in sentences:\n",
    "                    data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save DF\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethancavill/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "JOBLIB_DIR = Path.cwd() / \"aajonus_joblibs\"\n",
    "JOBLIB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "vectorizer_path = JOBLIB_DIR / 'tfidf_vectorizer.joblib'\n",
    "matrix_path = JOBLIB_DIR / 'tfidf_matrix.joblib'\n",
    "\n",
    "max_df = 0.90\n",
    "min_df = 0.00\n",
    "ngram_range = (1, 3)\n",
    "version=10\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "# Check if parameters have changed and files exist\n",
    "params_changed = False\n",
    "if vectorizer_path.exists():\n",
    "    existing_vectorizer = joblib.load(vectorizer_path)\n",
    "    if (existing_vectorizer.max_df != max_df or \n",
    "        existing_vectorizer.min_df != min_df or \n",
    "        existing_vectorizer.ngram_range != ngram_range):\n",
    "        params_changed = True\n",
    "        os.remove(vectorizer_path)\n",
    "        os.remove(matrix_path)\n",
    "\n",
    "if not matrix_path.exists() or params_changed:\n",
    "    print(\"Fitting TF-IDF vectorizer to the dataset...\")\n",
    "    vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['sentence'])\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    joblib.dump(tfidf_matrix, matrix_path)\n",
    "else:\n",
    "    print(\"Loading fitted TF-IDF vectorizer and matrix dataset...\")\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "    tfidf_matrix = joblib.load(matrix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search(query, vectorizer, tfidf_matrix, df):\n",
    "    query_vector = vectorizer.transform([query])  # Preprocessing is handled by vectorizer\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "    top_indices = similarities.argsort()[0][-3:]\n",
    "\n",
    "    # Retrieve the corresponding rows from the DataFrame\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    top_scores = similarities[0][top_indices]\n",
    "\n",
    "    return top_docs, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Relevance Score\", \"Filename\", \"Date\", \"Max DF\", \"Min DF\", \"Ngram Range\"]\n",
    "\n",
    "def search_main(query, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range, test_set):\n",
    "    top_docs, top_scores = search(query, vectorizer, tfidf_matrix, df)\n",
    "    \n",
    "    if top_docs.empty:\n",
    "        print(\"No documents found for this query.\")\n",
    "        return test_set\n",
    "\n",
    "    new_rows = []\n",
    "    for (index, row), score in zip(top_docs.iterrows(), top_scores):\n",
    "        new_row = {\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "            \"Relevance Score\": 0,  # Placeholder for manual scoring\n",
    "            \"Date\": pd.Timestamp('now'),\n",
    "            \"Max DF\": max_df,\n",
    "            \"Min DF\": min_df,\n",
    "            \"Ngram Range\": ngram_range\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    # Create a DataFrame from the new_rows list\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Append the new_rows_df to the test_set\n",
    "    test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "61276110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set_from_queries(query_file_path, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range):\n",
    "    test_set = pd.DataFrame(columns=test_set_columns)\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        queries = file.read().splitlines()\n",
    "    \n",
    "    for query in queries:\n",
    "        test_set = search_main(query, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range, test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "TEST_SET_DIR = Path.cwd() / \"aajonus_test_sets\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}.csv\"\n",
    "query_file_path = TEST_SET_DIR / \"queries.txt\"\n",
    "\n",
    "test_set = generate_test_set_from_queries(query_file_path, vectorizer, tfidf_matrix, df, max_df, min_df, ngram_range)\n",
    "\n",
    "test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ethancavill/Documents/code/nlp/aajonus_project/aajonus_test_sets/test_set_v10_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m EVAL_DIR\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m test_set_path \u001b[38;5;241m=\u001b[39m TEST_SET_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_set_v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scored.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m test_set \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_set_path)\n\u001b[1;32m     54\u001b[0m eval_df \u001b[38;5;241m=\u001b[39m compute_evaluations(test_set)\n\u001b[1;32m     55\u001b[0m eval_path \u001b[38;5;241m=\u001b[39m EVAL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ethancavill/Documents/code/nlp/aajonus_project/aajonus_test_sets/test_set_v10_scored.csv'"
     ]
    }
   ],
   "source": [
    "# After creating the test_set and evaluating the relevance of the top 5, we now need to compute some metrics. \n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_evaluations(test_set, threshold=0.5):\n",
    "    evaluation_data = []\n",
    "    \n",
    "    # This loops over each unique column name\n",
    "    for query in test_set['Query'].unique():\n",
    "        # Then we create a new df which filters the test_set on the query col\n",
    "        # This df will be 5 rows\n",
    "        current_query_data = test_set[test_set['Query'] == query]\n",
    "        \n",
    "        # We then extract out the columns and create numpy arrays from the respective values \n",
    "        true_relevance = current_query_data['Relevance Score'].to_numpy()\n",
    "        cosine_scores = current_query_data['Cosine'].to_numpy()\n",
    "        \n",
    "        # Convert cosine scores to binary predictions\n",
    "        predicted_relevance = (cosine_scores >= threshold).astype(int)\n",
    "        \n",
    "        # print(f\"True Relevance: {true_relevance}\")\n",
    "        # print(f\"Cosine scores: {cosine_scores}\")\n",
    "        # print(f\"Predicted Relevance: {predicted_relevance}\")\n",
    "        \n",
    "        # Calculate precision, recall, and F1-score with zero_division parameter\n",
    "        # Precison: True Pos / (True Pos + False Pos)\n",
    "        # Recall: True Pos / (True Pos + False Neg)\n",
    "        # Harmonic mean: 2 * (Prec x Rec / (Prec + Rec))\n",
    "        precision = precision_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        recall = recall_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        f1 = f1_score(true_relevance, predicted_relevance, zero_division=0)\n",
    "        \n",
    "        # print(f\"Precision: {precision}\")\n",
    "        # print(f\"Recall: {recall}\")\n",
    "        # print(f\"F1: {f1}\")\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query': query,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"aajonus_evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}_scored.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set)\n",
    "eval_path = EVAL_DIR / f\"evaluation_v{version}.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1619ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee76809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
