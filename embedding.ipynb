{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084335a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e05e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x17070a6d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3eb4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from CSV...\n",
      "                                            filename  \\\n",
      "0  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "1  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "2  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "3  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "4  Needles_Of_Disease_and_Death_Continue_In_The_N...   \n",
      "\n",
      "                                            sentence  \n",
      "0  On Halloween, I received the most alarming ter...  \n",
      "1  I received it\\nin a letter from Care2 organiza...  \n",
      "2  Most of us have\\nnever witnessed the crippling...  \n",
      "3  Polio is still endemic in three of the world's...  \n",
      "4  This is the scary truth: levels of polio are a...  \n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path.cwd() / \"aajonus_data\"\n",
    "\n",
    "DF_DIR = Path.cwd() / \"aajonus_saved_dfs\"\n",
    "DF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = DF_DIR / \"dataframe.csv\"\n",
    "\n",
    "# Conditional that checks whether we saved the dfs as csv files\n",
    "# If yes, then reinitialise these as dfs\n",
    "# If not, then create the dfs and save them in csv format for next run\n",
    "if df_path.exists():\n",
    "    print(\"Loading dataset from CSV...\")\n",
    "    df = pd.read_csv(df_path)\n",
    "else:\n",
    "    data = []\n",
    "\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "\n",
    "            # Create the filepath\n",
    "            file_path = DATA_DIR / filename\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                content = file.read()\n",
    "                # Use spaCy to tokenize the content into sentences\n",
    "                doc = nlp(content)\n",
    "                sentences = [sent.text.strip() for sent in doc.sents]\n",
    "                # Append each sentence to your data list, along with the filename\n",
    "                for sentence in sentences:\n",
    "                    data.append({\"filename\": filename, \"sentence\": sentence})\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save DF\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f929233f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69c2aa276cc49dea4d7715bcb05b6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "version = 1\n",
    "\n",
    "EMBEDDING_DIR = Path.cwd() / \"aajonus_embeddings\"\n",
    "EMBEDDING_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "embeddings_path = EMBEDDING_DIR / f'sentence_embeddings_v{version}.joblib'\n",
    "\n",
    "# Generate embeddings for the dataframe and save them\n",
    "if not embeddings_path.exists():\n",
    "    print(\"Generating embeddings for the dataset...\")\n",
    "    embeddings = model.encode(df['sentence'].tolist(), show_progress_bar=True)\n",
    "    joblib.dump(embeddings, embeddings_path)\n",
    "else:\n",
    "    print(\"Loading embeddings from file...\")\n",
    "    embeddings = joblib.load(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd05dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def search(query, embeddings, df):\n",
    "    # Encode the query to get its embedding\n",
    "    query_embedding = model.encode([query]) \n",
    "\n",
    "    # Calculate cosine similarity between the query embedding and all sentence embeddings\n",
    "    similarities = cosine_similarity(query_embedding, embeddings)[0]\n",
    "    \n",
    "    # Get the top 20 indices sorted by highest similarity scores\n",
    "    top_indices = np.argsort(similarities)[-20:]\n",
    "\n",
    "    # Retrieve the corresponding rows from the DataFrame\n",
    "    top_docs = df.iloc[top_indices]\n",
    "    top_scores = similarities[top_indices]\n",
    "\n",
    "    return top_docs, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a95e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_set_columns=[\"Query\", \"Result\", \"Cosine\", \"Filename\", \"Date\"]\n",
    "\n",
    "def search_main(query, embeddings, df, test_set):\n",
    "    top_docs, top_scores = search(query, embeddings, df)\n",
    "    \n",
    "    if top_docs.empty:\n",
    "        print(\"No documents found for this query.\")\n",
    "        return test_set\n",
    "\n",
    "    new_rows = []\n",
    "    for (index, row), score in zip(top_docs.iterrows(), top_scores):\n",
    "        new_row = {\n",
    "            \"Query\": query,\n",
    "            \"Result\": row['sentence'],\n",
    "            \"Cosine\": score,\n",
    "            \"Filename\": row['filename'],\n",
    "            \"Date\": pd.Timestamp('now'),\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    # Create a DataFrame from the new_rows list\n",
    "    new_rows_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Append the new_rows_df to the test_set\n",
    "    test_set = pd.concat([test_set, new_rows_df], ignore_index=True)\n",
    "    \n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61276110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_set_from_queries(query_file_path, embeddings, df):\n",
    "    test_set = pd.DataFrame(columns=test_set_columns)\n",
    "    with open(query_file_path, 'r') as file:\n",
    "        queries = file.read().splitlines()\n",
    "    \n",
    "    for query in queries:\n",
    "        test_set = search_main(query, embeddings, df, test_set)\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "TEST_SET_DIR = Path.cwd() / \"aajonus_test_sets\"\n",
    "TEST_SET_DIR.mkdir(exist_ok=True)\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}_embedding.csv\"\n",
    "query_file_path = TEST_SET_DIR / \"queries.txt\"\n",
    "\n",
    "test_set = generate_test_set_from_queries(query_file_path, embeddings, df)\n",
    "\n",
    "test_set.to_csv(test_set_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae5b34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Query Group  Total Hits  \\\n",
      "0  Is salt unhealthy, Salt damages cells,\\n Why i...           9   \n",
      "1    What are signs of intelligence, Genius and diet           3   \n",
      "2                        How to gain weight quickly            0   \n",
      "3     What is arthritis, What is arthritis caused by           1   \n",
      "4          What does high meat do, Why eat high meat           2   \n",
      "\n",
      "                                  Matching Sentences  \n",
      "0  some people are very allergic to salt, some pe...  \n",
      "1  being rational does not make you intelligent o...  \n",
      "2                                                     \n",
      "3  ten percent of arthritis is from other toxins ...  \n",
      "4  so thats what high meat can do for you, so tha...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_sentence(sentence):\n",
    "    # Function to normalize sentences for comparison\n",
    "    # Remove punctuation and extra spaces, and convert to lowercase\n",
    "    return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', sentence)).strip().lower()\n",
    "\n",
    "def compute_evaluations(test_set, relevant_results):\n",
    "    evaluation_data = []\n",
    "\n",
    "    for query_group in relevant_results['Query'].unique():\n",
    "        grouped_queries = query_group.split(',')\n",
    "        relevant_set = set([normalize_sentence(sentence) for sentence in relevant_results[relevant_results['Query'] == query_group]['Result']])\n",
    "        \n",
    "        total_hits = 0\n",
    "        matching_sentences = []\n",
    "\n",
    "        for query in grouped_queries:\n",
    "            query = query.strip()\n",
    "            query_results = test_set[test_set['Query'] == query]['Result'].apply(normalize_sentence)\n",
    "\n",
    "            # Count matching sentences, including duplicates\n",
    "            for sentence in query_results:\n",
    "                if sentence in relevant_set:\n",
    "                    total_hits += 1\n",
    "                    matching_sentences.append(sentence)\n",
    "\n",
    "        evaluation_data.append({\n",
    "            'Query Group': query_group,\n",
    "            'Total Hits': total_hits,\n",
    "            'Matching Sentences': ', '.join(matching_sentences)\n",
    "        })\n",
    "    \n",
    "    eval_df = pd.DataFrame(evaluation_data)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "EVAL_DIR = Path.cwd() / \"aajonus_evaluations\"\n",
    "EVAL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "relevant_results_path = TEST_SET_DIR / \"relevant_query_results.csv\"\n",
    "relevant_results = pd.read_csv(relevant_results_path)\n",
    "\n",
    "test_set_path = TEST_SET_DIR / f\"test_set_v{version}_embedding.csv\"\n",
    "test_set = pd.read_csv(test_set_path)\n",
    "\n",
    "eval_df = compute_evaluations(test_set, relevant_results)\n",
    "eval_path = EVAL_DIR / f\"evaluation_v{version}_embedding.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(eval_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f286df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177e185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
